"""
AI æœåŠ¡ - æ··åˆæ¨¡å‹ä¼˜åŒ–ç‰ˆæœ¬
ä½œè€…çµæ„Ÿæ¥æºï¼šä¹”å¸ƒæ–¯çš„ç®€çº¦å“²å­¦ + å¼ å°é¾™çš„å…‹åˆ¶è®¾è®¡

ğŸ”¥ æœ€æ–°æ›´æ–°ï¼š
1. AI æš–å¿ƒåé¦ˆä» Claude Sonnet å›å½’ OpenAI GPT-4o-miniï¼ˆTestFlight éªŒè¯ç¨³å®šç‰ˆï¼‰
2. æ¶¦è‰² + æ ‡é¢˜ä¸åé¦ˆç»Ÿä¸€ä½¿ç”¨ GPT-4o-miniï¼ˆé™ä½ç»´æŠ¤æˆæœ¬ï¼‰
3. å¹¶è¡Œæ‰§è¡Œç­–ç•¥ä¿æŒä¸å˜ï¼Œæ€§èƒ½ç»§ç»­ç¨³å®š
4. Whisper è¯­éŸ³è½¬æ–‡å­—æŒç»­æ²¿ç”¨ï¼Œä¿è¯è¯†åˆ«å‡†ç¡®åº¦

æ ¸å¿ƒç†å¿µï¼š
1. ç®€å•ä½†ä¸ç®€é™‹ï¼ˆSimple but not simplisticï¼‰
2. å¼ºå¤§ä½†ä¸å¤æ‚ï¼ˆPowerful but not complicatedï¼‰
3. ä¼˜é›…ä½†ä¸ç‚«æŠ€ï¼ˆElegant but not showyï¼‰
"""

import tempfile
import os
import json
import asyncio  # ğŸ”¥ ç”¨äºå¹¶è¡Œæ‰§è¡Œ
from typing import Dict, Optional, List
from openai import OpenAI
import io
import base64
import requests

from ..config import get_settings


class OpenAIService:
    """
    AI æœåŠ¡ç±» - æ”¯æŒå¤šè¯­è¨€æ—¥è®°å¤„ç†
    
    è¿™ä¸ªç±»å°±åƒä¸€ä¸ªæ¸©æŸ”çš„æ—¥è®°åŠ©æ‰‹ï¼Œå®ƒä¼šï¼š
    1. å¬æ‡‚ä½ çš„å£°éŸ³ï¼ˆè¯­éŸ³è½¬æ–‡å­— - Whisperï¼‰
    2. ç¾åŒ–ä½ çš„æ–‡å­—ï¼ˆè½»åº¦æ¶¦è‰² - GPT-4o-miniï¼‰
    3. ç»™ä½ æ¸©æš–çš„å›åº”ï¼ˆå¿ƒç†é™ªä¼´ - GPT-4o-miniï¼‰
    4. å¸®ä½ èµ·ä¸ªå¥½æ ‡é¢˜ï¼ˆç”»é¾™ç‚¹ç› - GPT-4o-miniï¼‰
    
    ğŸ”¥ æ¨¡å‹é€‰æ‹©ç­–ç•¥ï¼š
    - Whisper: è¯­éŸ³è½¬æ–‡å­—ï¼ˆOpenAIï¼Œæ— å¯æ›¿ä»£ï¼‰
    - GPT-4o-mini: æ¶¦è‰² + æ ‡é¢˜ï¼ˆå¿«é€Ÿã€ç¨³å®šã€æˆæœ¬å¯æ§ï¼‰
    - GPT-4o-mini: AI åé¦ˆï¼ˆTestFlight å›å½’éªŒè¯æ›´ç¨³å®šï¼‰
    """
    
    # ğŸ¯ æ¨¡å‹é…ç½®
    MODEL_CONFIG = {
        # è¯­éŸ³è½¬æ–‡å­—ï¼ˆä¿æŒä¸å˜ï¼‰
        "transcription": "whisper-1",
        
        # ğŸ”¥ GPT æ¨¡å‹é…ç½®
        "haiku": "gpt-4o-mini",  # æ¶¦è‰² + æ ‡é¢˜ï¼ˆå‘½åæ²¿ç”¨æ—§å­—æ®µï¼Œä¾¿äºå…¼å®¹ï¼‰
        "sonnet": "gpt-4o-mini",  # AI æš–å¿ƒåé¦ˆï¼ˆå›å½’ OpenAI æ¨¡å‹ï¼‰
        
        # ğŸ¤ ä¸ºä»€ä¹ˆ Whisperï¼Ÿ
        # âœ… OpenAI å®˜æ–¹è¯­éŸ³è½¬æ–‡å­—æ¨¡å‹
        # âœ… æ”¯æŒ 100+ è¯­è¨€ï¼ˆä¸­è‹±æ–‡å®Œç¾ï¼‰
        # âœ… é«˜å‡†ç¡®åº¦ï¼Œä½å¹»è§‰ç‡
        
        # ğŸ¨ ä¸ºä»€ä¹ˆ Haiku æ¶¦è‰²ï¼Ÿ
        # âœ… é€Ÿåº¦å¿«ï¼ˆ1-2ç§’ï¼‰
        # âœ… ä¾¿å®œï¼ˆ$1/1M tokens inputï¼‰
        # âœ… è¶³å¤Ÿèªæ˜ï¼ˆæ—¥è®°æ¶¦è‰²ç»°ç»°æœ‰ä½™ï¼‰
        
        # ğŸ’¬ ä¸ºä»€ä¹ˆ GPT-4o-mini åé¦ˆï¼Ÿ
        # âœ… æ¸©æš–çœŸå®ï¼ˆå…¼é¡¾å…±æƒ…ä¸å®‰å…¨ï¼‰
        # âœ… å¤šè¯­è¨€èƒ½åŠ›å¼ºï¼ˆä¸­è‹±æ–‡éƒ½è‡ªç„¶ï¼‰
        # âœ… ä¸æ¶¦è‰²æ¨¡å‹ç»Ÿä¸€ï¼Œæ–¹ä¾¿ç»´æŠ¤
    }
    
    # ğŸ“ é•¿åº¦é™åˆ¶ï¼ˆä¿æŒä¸å˜ï¼‰
    LENGTH_LIMITS = {
        "title_min": 4,
        "title_max": 50,
        "feedback_min": 30,
        "feedback_max": 250,
        "polished_ratio": 1.15,
        "min_audio_text": 5,
    }
    
    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡å®¢æˆ·ç«¯"""
        settings = get_settings()
        
        # OpenAI å®¢æˆ·ç«¯ï¼ˆç”¨äº Whisperï¼‰
        self.openai_client = OpenAI(api_key=settings.openai_api_key)
        self.openai_api_key = settings.openai_api_key
        
        print(f"âœ… AI æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        print(f"   - Whisper: è¯­éŸ³è½¬æ–‡å­—")
        print(f"   - GPT-4o-mini: æ¶¦è‰² + æ ‡é¢˜ (é…ç½®å­—æ®µ haiku)")
        print(f"   - GPT-4o-mini: AI åé¦ˆ (é…ç½®å­—æ®µ sonnet)")
    
    # ========================================================================
    # è¯­éŸ³è½¬æ–‡å­—ï¼ˆä¿æŒä¸å˜ï¼‰
    # ========================================================================
    
    async def transcribe_audio(
        self, 
        audio_content: bytes, 
        filename: str,
        expected_duration: Optional[int] = None
    ) -> str:
        """
        è¯­éŸ³è½¬æ–‡å­— - æŠŠä½ çš„å£°éŸ³å˜æˆæ–‡å­—
        
        ğŸ”¥ æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•å®Œå…¨ä¸å˜ï¼Œç»§ç»­ä½¿ç”¨ Whisper
        
        å·¥ä½œæµç¨‹ï¼š
        1. æ”¶åˆ°éŸ³é¢‘ â†’ æ£€æŸ¥å¤§å°
        2. åˆ›å»ºä¸´æ—¶æ–‡ä»¶ â†’ ç¡®ä¿æ ¼å¼æ­£ç¡®
        3. å‘é€ç»™ Whisper â†’ å®ƒæ˜¯è¯­éŸ³è¯†åˆ«ä¸“å®¶
        4. æ£€æŸ¥ç»“æœ â†’ ç¡®ä¿ä¸æ˜¯ç©ºçš„
        5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶ â†’ ä¿æŒæ•´æ´
        """
        temp_file_path = None
        
        try:
            # æ£€æŸ¥éŸ³é¢‘å¤§å°
            audio_size_kb = len(audio_content) / 1024
            print(f"ğŸ¤ æ”¶åˆ°éŸ³é¢‘: {filename}, å¤§å°: {audio_size_kb:.1f} KB")
            
            if audio_size_kb < 1:
                raise ValueError("éŸ³é¢‘æ–‡ä»¶å¤ªå°ï¼Œè¯·è¯´é•¿ä¸€ç‚¹")
            
            # å‡†å¤‡ä¸´æ—¶æ–‡ä»¶
            suffix = '.m4a' if not filename.endswith('.m4a') else ''
            with tempfile.NamedTemporaryFile(
                delete=False, 
                suffix=suffix or os.path.splitext(filename)[1]
            ) as temp_file:
                temp_file.write(audio_content)
                temp_file_path = temp_file.name
            
            print(f"âœ… ä¸´æ—¶æ–‡ä»¶å‡†å¤‡å®Œæˆ")
            
            # è°ƒç”¨ Whisper
            import httpx
            import io
            print("ğŸ“¤ æ­£åœ¨è¯†åˆ«è¯­éŸ³ï¼ˆverbose_json æ¨¡å¼ï¼‰...")
            response_json = None
            try:
                with httpx.Client(timeout=60.0) as client:
                    file_stream = io.BytesIO(audio_content)
                    response = client.post(
                        "https://api.openai.com/v1/audio/transcriptions",
                        headers={
                            "Authorization": f"Bearer {self.openai_api_key}",
                        },
                        data={
                            "model": self.MODEL_CONFIG["transcription"],
                            "language": "",
                            "temperature": "0",
                            "response_format": "verbose_json",
                        },
                        files={
                            "file": (filename or "recording.m4a", file_stream, "audio/m4a"),
                        },
                    )
                    response.raise_for_status()
                    response_json = response.json()
            except httpx.HTTPError as http_err:
                print(f"âŒ Whisper HTTP è¯·æ±‚å¤±è´¥: {http_err}")
                if http_err.response is not None:
                    print(f"ğŸ“„ Whisper å“åº”: {http_err.response.text[:200]}...")
                raise ValueError("è¯­éŸ³è¯†åˆ«å¤±è´¥: æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•")
            
            if not response_json:
                raise ValueError("è¯­éŸ³è¯†åˆ«å¤±è´¥: æœªæ”¶åˆ°æœ‰æ•ˆå“åº”")
            
            text = (response_json.get("text") or "").strip()
            segments = response_json.get("segments", []) or []
            
            import re
            normalized_text = re.sub(r"\s+", "", text)
            
            if len(normalized_text) < self.LENGTH_LIMITS["min_audio_text"]:
                print(f"âŒ è½¬å½•å†…å®¹è¿‡çŸ­: '{text}'")
                raise ValueError("æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè¯·è¯´æ¸…æ¥šä¸€äº›")
            
            filler_tokens = {
                "um",
                "uh",
                "uhh",
                "hmm",
                "hmmm",
                "erm",
                "er",
                "ah",
                "oh",
                "mmm",
            }
            token_pattern = r"[A-Za-z\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff]+"
            tokens = re.findall(token_pattern, text)
            meaningful_tokens = [
                token
                for token in tokens
                if len(token) >= 2 and token.lower() not in filler_tokens
            ]
            cjk_chars = re.findall(r"[\u4e00-\u9fff]", text)
            has_cjk = len(cjk_chars) > 0
            
            unique_chars = len(set(normalized_text))
            if unique_chars <= 2 and len(normalized_text) > 2:
                print(
                    "âŒ è½¬å½•ç»“æœåŒ…å«å¤§é‡é‡å¤å­—ç¬¦ï¼Œè§†ä¸ºæ— æ•ˆ:",
                    {"text": text, "normalized": normalized_text},
                )
                raise ValueError("æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè¯·è¯´æ¸…æ¥šä¸€äº›")
            
            # åˆ†æ Whisper æ®µç»“æœï¼Œç¡®è®¤æ˜¯å¦çœŸçš„æœ‰è®²è¯
            def _segment_value(segment, attr, default):
                if isinstance(segment, dict):
                    return segment.get(attr, default)
                return getattr(segment, attr, default)
            
            confident_segments = []
            total_confident_duration = 0.0
            total_segment_duration = 0.0
            avg_no_speech_sum = 0.0
            
            for segment in segments:
                try:
                    start = float(_segment_value(segment, "start", 0))
                    end = float(_segment_value(segment, "end", 0))
                    seg_duration = max(0.0, end - start)
                except (TypeError, ValueError):
                    seg_duration = 0.0
                    start = 0.0
                    end = 0.0
                
                total_segment_duration += seg_duration
                
                try:
                    no_speech_prob = float(_segment_value(segment, "no_speech_prob", 1))
                except (TypeError, ValueError):
                    no_speech_prob = 1
                
                try:
                    avg_logprob = float(_segment_value(segment, "avg_logprob", -10))
                except (TypeError, ValueError):
                    avg_logprob = -10
                
                avg_no_speech_sum += no_speech_prob * seg_duration
                
                if (
                    seg_duration >= 0.3
                    and no_speech_prob < 0.45
                    and avg_logprob > -0.75
                ):
                    confident_segments.append(segment)
                    total_confident_duration += seg_duration
            
            reference_duration = None
            if expected_duration and expected_duration > 0:
                reference_duration = float(expected_duration)
            elif total_segment_duration > 0:
                reference_duration = total_segment_duration
            else:
                reference_duration = None
            
            speech_ratio = (
                total_confident_duration / reference_duration
                if reference_duration and reference_duration > 0
                else None
            )
            
            avg_no_speech_prob = (
                avg_no_speech_sum / total_segment_duration
                if total_segment_duration > 0
                else 1.0
            )
            
            if reference_duration and reference_duration >= 6:
                if (
                    len(normalized_text) < self.LENGTH_LIMITS["min_audio_text"]
                    and (speech_ratio is None or speech_ratio < 0.15)
                    and total_confident_duration < 0.6
                ):
                    print(
                        "âŒ æ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³è¿‡å°‘:",
                        {
                            "expected_duration": expected_duration,
                            "total_confident_duration": total_confident_duration,
                            "speech_ratio": speech_ratio,
                            "avg_no_speech_prob": avg_no_speech_prob,
                            "segments_count": len(segments),
                        },
                    )
                    raise ValueError("æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè¯·è¯´æ¸…æ¥šä¸€äº›")
            
            # å¯¹é•¿å½•éŸ³ä¸å†ä½¿ç”¨å­—ç¬¦å¯†åº¦ç¡¬é˜ˆå€¼ï¼Œé¿å…è¯¯æ€çœŸå®å†…å®¹

            if reference_duration:
                if has_cjk:
                    # ä¸­æ–‡åœºæ™¯ï¼šç”¨æ±‰å­—æ•°é‡åˆ¤æ–­ï¼Œé¿å…â€œä¸€ä¸ªé•¿è¯â€è¢«è¯¯åˆ¤
                    if (
                        len(cjk_chars) < 3
                        and len(normalized_text) < self.LENGTH_LIMITS["min_audio_text"]
                    ):
                        print(
                            "âŒ ä¸­æ–‡æœ‰æ•ˆå­—ç¬¦è¿‡å°‘ï¼Œåˆ¤å®šä¸ºæ— æ„ä¹‰å†…å®¹:",
                            {
                                "cjk_chars": len(cjk_chars),
                                "duration": reference_duration,
                            },
                        )
                        raise ValueError("æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè¯·ç¨ä½œè¡¨è¾¾åå†è¯•")
                else:
                    if (
                        len(meaningful_tokens) < 2
                        and len(normalized_text) < self.LENGTH_LIMITS["min_audio_text"] * 2
                    ):
                        print(
                            "âŒ æœ‰æ•ˆè¯æ±‡æ•°é‡ä¸è¶³ï¼Œåˆ¤å®šä¸ºæ— æ„ä¹‰å†…å®¹:",
                            {
                                "tokens": tokens,
                                "meaningful_tokens": meaningful_tokens,
                                "duration": reference_duration,
                            },
                        )
                        raise ValueError("æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè¯·ç¨ä½œè¡¨è¾¾åå†è¯•")
            
            print(f"âœ… è¯­éŸ³è¯†åˆ«æˆåŠŸ: '{text[:50]}...'")
            return text
            
        except Exception as e:
            print(f"âŒ è¯­éŸ³è½¬æ–‡å­—å¤±è´¥: {str(e)}")
            if "Invalid file format" in str(e):
                raise ValueError("éŸ³é¢‘æ ¼å¼ä¸æ”¯æŒï¼Œè¯·ä½¿ç”¨ m4a æ ¼å¼")
            elif "File too large" in str(e):
                raise ValueError("éŸ³é¢‘æ–‡ä»¶å¤ªå¤§ï¼Œè¯·æ§åˆ¶åœ¨ 2 åˆ†é’Ÿå†…")
            else:
                raise ValueError(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file_path and os.path.exists(temp_file_path):
                try:
                    os.unlink(temp_file_path)
                    print(f"ğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
                except Exception as e:
                    print(f"âš ï¸ æ¸…ç†å¤±è´¥ï¼ˆä¸å½±å“åŠŸèƒ½ï¼‰: {e}")
    
    # ========================================================================
    # ğŸ”¥ æ ¸å¿ƒæ”¹åŠ¨ï¼šæ··åˆæ¨¡å‹å¤„ç†
    # ========================================================================
    
    async def polish_content_multilingual(
        self, 
        text: str,
        user_name: Optional[str] = None,  # ç”¨æˆ·åå­—ï¼Œç”¨äºä¸ªæ€§åŒ–åé¦ˆ
        image_urls: Optional[List[str]] = None  # å›¾ç‰‡URLåˆ—è¡¨ï¼Œç”¨äºvisionåˆ†æ
    ) -> Dict[str, str]:
        """
        ğŸ”¥ é‡å¤§æ”¹åŠ¨ï¼šä»å•ä¸€æ¨¡å‹æ”¹ä¸ºæ··åˆæ¨¡å‹ + å¹¶è¡Œæ‰§è¡Œ
        
        æ—§é€»è¾‘ï¼š
        1. GPT-4o-mini ä¸€æ¬¡æ€§ç”Ÿæˆæ¶¦è‰² + æ ‡é¢˜ + åé¦ˆï¼ˆä¸²è¡Œï¼Œ3-5ç§’ï¼‰
        
        æ–°é€»è¾‘ï¼š
        1. GPT-4o-mini ç”Ÿæˆæ¶¦è‰² + æ ‡é¢˜ï¼ˆå­—æ®µ haikuï¼Œ1-2ç§’ï¼‰
        2. GPT-4o-mini ç”Ÿæˆåé¦ˆï¼ˆå­—æ®µ sonnetï¼ŒåŸºäºåŸå§‹æ–‡æœ¬ï¼Œ2-3ç§’ï¼‰
        3. ä¸¤ä¸ªä»»åŠ¡å¹¶è¡Œæ‰§è¡Œï¼Œæ€»è€—æ—¶ = max(1-2, 2-3) = 2-3ç§’
        
        ä¸ºä»€ä¹ˆåŸºäºåŸå§‹æ–‡æœ¬ç”Ÿæˆåé¦ˆï¼Ÿ
        - æ›´çœŸå®ï¼šåŸå§‹æ–‡æœ¬ä¿ç•™äº†ç”¨æˆ·æœ€çœŸå®çš„æƒ…æ„Ÿ
        - æ›´å¿«ï¼šä¸éœ€è¦ç­‰æ¶¦è‰²å®Œæˆ
        - æ›´æ¸©æš–ï¼šAI å›åº”"çœŸå®çš„ä½ "è€Œä¸æ˜¯"å®Œç¾çš„æ–‡å­—"
        """
        try:
            # è¾“å…¥æ£€æŸ¥
            if not text or len(text.strip()) < 5:
                raise ValueError("å†…å®¹å¤ªçŸ­ï¼Œè¯·å¤šå†™ä¸€äº›")
            
            print(f"âœ¨ å¼€å§‹AIå¤„ç†ï¼ˆå¹¶è¡Œæ¨¡å¼ï¼‰: {text[:50]}...")
            
            # ğŸ”¥ ä¼˜åŒ–è¯­è¨€æ£€æµ‹ï¼šæ›´å‡†ç¡®åœ°è¯†åˆ«ç”¨æˆ·è¾“å…¥çš„ä¸»è¦è¯­è¨€
            import re
            # ç§»é™¤ç©ºç™½å­—ç¬¦å’Œæ ‡ç‚¹ï¼Œåªç»Ÿè®¡å®é™…å†…å®¹å­—ç¬¦
            content_only = re.sub(r'[\s\W]', '', text)
            chinese_chars = 0
            english_words = 0
            
            if not content_only:
                # å¦‚æœåªæœ‰ç©ºç™½å’Œæ ‡ç‚¹ï¼Œé»˜è®¤ä½¿ç”¨ä¸­æ–‡
                detected_lang = "Chinese"
            else:
                # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
                chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content_only))
                # ç»Ÿè®¡è‹±æ–‡å­—ç¬¦ï¼ˆå•è¯ï¼‰
                english_words = len(re.findall(r'[a-zA-Z]+', content_only))
                # è®¡ç®—ä¸­æ–‡å­—ç¬¦å æ¯”
                chinese_ratio = chinese_chars / len(content_only) if len(content_only) > 0 else 0
                # è®¡ç®—è‹±æ–‡å•è¯å æ¯”ï¼ˆæ¯ä¸ªå•è¯å¹³å‡5ä¸ªå­—ç¬¦ä¼°ç®—ï¼‰
                english_ratio = (english_words * 5) / len(content_only) if len(content_only) > 0 else 0
                
                # ğŸ”¥ å…³é”®é€»è¾‘ï¼šå¦‚æœä¸­æ–‡å­—ç¬¦å æ¯”è¶…è¿‡30%ï¼Œæˆ–è€…ä¸­æ–‡å­—ç¬¦æ•°é‡æ˜æ˜¾å¤šäºè‹±æ–‡å•è¯ï¼Œåˆ¤å®šä¸ºä¸­æ–‡
                # è¿™æ ·å¯ä»¥é¿å…"å°‘é‡ä¸­æ–‡+å¤§é‡è‹±æ–‡"è¢«è¯¯åˆ¤ä¸ºè‹±æ–‡çš„æƒ…å†µ
                if chinese_ratio > 0.3 or (chinese_chars > 5 and chinese_chars > english_words * 2):
                    detected_lang = "Chinese"
                elif english_ratio > 0.5 or english_words > 10:
                    detected_lang = "English"
                else:
                    # é»˜è®¤ï¼šå¦‚æœä¸­æ–‡å­—ç¬¦å­˜åœ¨ä¸”æ•°é‡>=3ï¼Œåˆ¤å®šä¸ºä¸­æ–‡
                    detected_lang = "Chinese" if chinese_chars >= 3 else "English"
            
            print(f"ğŸŒ æ£€æµ‹åˆ°è¯­è¨€: {detected_lang} (ä¸­æ–‡å­—ç¬¦={chinese_chars}, è‹±æ–‡å•è¯={english_words})")
            
            # ğŸ”¥ å…³é”®æ”¹åŠ¨ï¼šå¹¶è¡Œæ‰§è¡Œä¸¤ä¸ªä»»åŠ¡
            print(f"ğŸš€ å¯åŠ¨å¹¶è¡Œå¤„ç†...")
            if image_urls and len(image_urls) > 0:
                print(f"   - æ£€æµ‹åˆ° {len(image_urls)} å¼ å›¾ç‰‡ï¼Œå°†ä½¿ç”¨ Vision èƒ½åŠ›åˆ†æå›¾ç‰‡+æ–‡å­—")
            print(f"   - ä»»åŠ¡1: GPT-4o-mini æ¶¦è‰² + æ ‡é¢˜ï¼ˆå­—æ®µ haikuï¼‰")
            print(f"   - ä»»åŠ¡2: GPT-4o-mini æš–å¿ƒåé¦ˆï¼ˆå­—æ®µ sonnetï¼ŒåŸºäºåŸå§‹æ–‡æœ¬ï¼‰")
            
            # ğŸ”¥ æ€§èƒ½ä¼˜åŒ–ï¼šé¢„å…ˆä¸‹è½½å¹¶ç¼–ç æ‰€æœ‰å›¾ç‰‡ï¼Œé¿å…åœ¨å¹¶è¡Œä»»åŠ¡ä¸­é‡å¤ä¸‹è½½
            encoded_images = []
            if image_urls and len(image_urls) > 0:
                print(f"ğŸ–¼ï¸ é¢„å¤„ç† {len(image_urls)} å¼ å›¾ç‰‡...")
                # å¹¶è¡Œä¸‹è½½å›¾ç‰‡
                download_tasks = [self._download_and_encode_image(url) for url in image_urls]
                results = await asyncio.gather(*download_tasks, return_exceptions=True)
                for i, img_data in enumerate(results):
                    if isinstance(img_data, Exception):
                        print(f"âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥ ({image_urls[i]}): {img_data}")
                    else:
                        encoded_images.append(img_data)
            
            # åˆ›å»ºä¸¤ä¸ªå¼‚æ­¥ä»»åŠ¡
            polish_task = self._call_gpt4o_mini_for_polish_and_title(text, detected_lang, encoded_images)
            feedback_task = self._call_gpt4o_mini_for_feedback(text, detected_lang, user_name, encoded_images)
            
            # å¹¶è¡Œæ‰§è¡Œå¹¶ç­‰å¾…ç»“æœ
            polish_result, feedback = await asyncio.gather(
                polish_task,
                feedback_task
            )
            
            print(f"âœ… å¹¶è¡Œå¤„ç†å®Œæˆ")
            
            # åˆå¹¶ç»“æœ
            result = {
                "title": polish_result['title'],
                "polished_content": polish_result['polished_content'],
                "feedback": feedback
            }
            
            # è´¨é‡æ£€æŸ¥
            result = self._validate_and_fix_result(result, text)
            
            print(f"âœ… å¤„ç†å®Œæˆ:")
            print(f"  - æ ‡é¢˜: {result['title']}")
            print(f"  - å†…å®¹é•¿åº¦: {len(result['polished_content'])} å­—")
            print(f"  - åé¦ˆé•¿åº¦: {len(result['feedback'])} å­—")
            
            return result
        
        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)
            print(f"âŒ AIå¤„ç†å¤±è´¥: {error_type}: {error_msg}")
            import traceback
            error_trace = traceback.format_exc()
            print(f"ğŸ“ å®Œæ•´é”™è¯¯å †æ ˆ:")
            print(error_trace)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¹¶è¡Œä»»åŠ¡ä¸­çš„é”™è¯¯
            if isinstance(e, (asyncio.TimeoutError, asyncio.CancelledError)):
                print(f"âš ï¸ å¹¶è¡Œä»»åŠ¡è¶…æ—¶æˆ–å–æ¶ˆ")
            elif isinstance(e, Exception):
                print(f"âš ï¸ å¹¶è¡Œä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            
            return self._create_fallback_result(text)
    
    # ========================================================================
    # ğŸ”¥ GPT-4o-mini è°ƒç”¨ï¼ˆæ¶¦è‰² + æ ‡é¢˜ï¼‰
    # ========================================================================
    
    async def _call_gpt4o_mini_for_polish_and_title(
        self, 
        text: str,
        language: str,
        encoded_images: Optional[List[str]] = None
    ) -> Dict[str, str]:
        """
        è°ƒç”¨ GPT-4o-mini è¿›è¡Œæ¶¦è‰²å’Œç”Ÿæˆæ ‡é¢˜
        
        ğŸ“š å­¦ä¹ ç‚¹ï¼šè¿™ä¸ªå‡½æ•°è´Ÿè´£ä¸¤ä¸ªä»»åŠ¡
        1. æ¶¦è‰²ç”¨æˆ·çš„åŸå§‹æ–‡æœ¬ï¼ˆä¿®å¤è¯­æ³•ã€ä¼˜åŒ–è¡¨è¾¾ï¼‰
        2. ç”Ÿæˆä¸€ä¸ªç®€æ´æœ‰æ„ä¹‰çš„æ ‡é¢˜
        
        ä¸ºä»€ä¹ˆä½¿ç”¨ GPT-4o-miniï¼Ÿ
        - é€Ÿåº¦å¿«ï¼ˆ1-2ç§’ï¼‰
        - æˆæœ¬ä½ï¼ˆ$1/1M tokens inputï¼‰
        - è´¨é‡è¶³å¤Ÿï¼ˆæ—¥è®°æ¶¦è‰²ç»°ç»°æœ‰ä½™ï¼‰
        
        è¿”å›:
            {
                "title": "æ ‡é¢˜",
                "polished_content": "æ¶¦è‰²åçš„å†…å®¹"
            }
        """
        try:
            print(f"ğŸ¨ GPT-4o-mini: å¼€å§‹æ¶¦è‰²å’Œç”Ÿæˆæ ‡é¢˜...")
            
            # ğŸ”¥ ä¼˜åŒ–ï¼šæ ¹æ®ä¼ å…¥çš„ language å‚æ•°æ„å»ºæ›´ä¸¥æ ¼çš„ prompt
            # æ ¸å¿ƒåŸåˆ™ï¼šæ ‡é¢˜è¯­è¨€å¿…é¡»ä¸ç”¨æˆ·è¾“å…¥å†…å®¹çš„ä¸»è¦è¯­è¨€å®Œå…¨ä¸€è‡´
            language_instruction = ""
            if language == "Chinese":
                language_instruction = """ğŸš¨ CRITICAL LANGUAGE RULE - YOU MUST FOLLOW:
The user's content is primarily in CHINESE (ç®€ä½“ä¸­æ–‡). 

MANDATORY REQUIREMENTS:
1. **Title MUST be in Chinese (ç®€ä½“ä¸­æ–‡) ONLY** - NO English, NO Japanese, NO Korean
2. **Title language must match the user's input language** - If user writes in Chinese, title MUST be Chinese
3. Even if the content contains some English words or other languages, the title MUST be in Chinese
4. Polished content should preserve the original language of each part, but the title MUST be Chinese

WRONG Examples (DO NOT DO THIS):
- User input in Chinese â†’ Title: "Reflections on..." âŒ
- User input in Chinese â†’ Title: "ã‚ªãƒ¬ãƒ³ã‚¸ã®é­…åŠ›" âŒ

CORRECT Examples:
- User input: "æˆ‘å…ˆè¯•ä¸€ä¸‹è¯­éŸ³è¾“å…¥ï¼Œç°åœ¨æ€ä¹ˆæ ·" â†’ Title: "è¯­éŸ³è¾“å…¥çš„å°è¯•" âœ…
- User input: "ã‚ªãƒ¬ãƒ³ã‚¸ã®é­…åŠ› Talking about orange..." â†’ Title: "æ©™å­çš„é­…åŠ›" âœ… (Chinese, not Japanese)"""
            elif language == "English":
                language_instruction = """ğŸš¨ CRITICAL LANGUAGE RULE - YOU MUST FOLLOW:
The user's content is primarily in ENGLISH.

MANDATORY REQUIREMENTS:
1. **Title MUST be in English ONLY** - NO Chinese, NO Japanese, NO Korean
2. **Title language must match the user's input language** - If user writes in English, title MUST be English
3. Even if the content contains some Chinese words or other languages, the title MUST be in English
4. Polished content should preserve the original language of each part, but the title MUST be English

WRONG Examples (DO NOT DO THIS):
- User input in English â†’ Title: "ä»Šæ—¥è®°å½•" âŒ
- User input in English â†’ Title: "ã‚ªãƒ¬ãƒ³ã‚¸ã®é­…åŠ›" âŒ

CORRECT Examples:
- User input: "today was good i went to park" â†’ Title: "A Day at the Park" âœ…
- User input: "ã‚ªãƒ¬ãƒ³ã‚¸ã®é­…åŠ› Talking about orange..." â†’ Title: "The Charm of Oranges" âœ… (English, not Japanese)"""
            else:
                # é»˜è®¤ï¼šæ£€æµ‹è¯­è¨€ï¼Œä½†å¿…é¡»ä¸¥æ ¼åŒ¹é…
                language_instruction = """ğŸš¨ CRITICAL LANGUAGE RULE - YOU MUST FOLLOW:
Detect the user's PRIMARY language from their input content.

MANDATORY REQUIREMENTS:
1. **Title language MUST match the user's primary input language**
2. If content is primarily Chinese â†’ Title MUST be Chinese
3. If content is primarily English â†’ Title MUST be English
4. If content contains mixed languages, use the language that appears MOST FREQUENTLY
5. NEVER use Japanese or Korean for titles unless the ENTIRE content is in that language
6. **DO NOT mix languages in the title** - Use ONE language only, matching the user's primary language

Examples:
- User input: "ä»Šå¤©å¤©æ°”å¾ˆå¥½" (Chinese) â†’ Title: "ç¾å¥½çš„å¤©æ°”" âœ… (Chinese)
- User input: "today was good" (English) â†’ Title: "A Good Day" âœ… (English)
- User input: "ä»Šå¤©å¤©æ°”å¾ˆå¥½ today was good" (mixed, more Chinese) â†’ Title: "ç¾å¥½çš„ä¸€å¤©" âœ… (Chinese, matching primary language)"""
            
            # æ„å»º prompt
            system_prompt = f"""You are a gentle diary editor. Your task is to polish the user's diary entry and create a title.

{language_instruction}

Your responsibilities:
1. Fix obvious grammar/typos
2. Make the text flow naturally
3. Keep it â‰¤115% of original length
4. **CRITICAL: Preserve ALL original content. Do NOT delete or omit any part of the user's entry.**
5. **Formatting: Preserve the user's line breaks, blank lines, and bullet/numbered lists. Do NOT merge everything into one paragraph.**
6. **If the input is long and mostly one block (no line breaks), add clear paragraph breaks based on meaning.**
7. **Avoid overly short paragraphs. Do NOT break right after the first sentence. Keep the first 3 sentences in the same paragraph when you add breaks.**
8. **ğŸš¨ MOST CRITICAL: Create a title in the EXACT SAME LANGUAGE as the user's primary input language**
   - If user writes in Chinese â†’ Title MUST be in Chinese
   - If user writes in English â†’ Title MUST be in English
   - The title language must match the content language - NO EXCEPTIONS
   - Title should be short, warm, poetic, and meaningful, but ALWAYS in the user's language

Style: Natural, warm, authentic. Don't over-edit.

Response format (JSON only):
{{
  "title": "Title in the EXACT SAME LANGUAGE as the user's primary input (Chinese or English only - MUST match user's language)",
  "polished_content": "fixed text, preserving original language AND original formatting (line breaks/lists) - MUST include all original content"
}}

ğŸš¨ CRITICAL EXAMPLES - Study these carefully:

Example 1 (User writes in Chinese - Title MUST be Chinese):
Input: "æˆ‘å…ˆè¯•ä¸€ä¸‹è¯­éŸ³è¾“å…¥ï¼Œç°åœ¨æ€ä¹ˆæ ·ã€‚å“å‘€ï¼Œå°±æ˜¯æœ‰ç‚¹å¤±è½ï¼Œå› ä¸ºæ˜æ˜åº”è¯¥æ—©ç‚¹ç¡çš„ã€‚"
Output: {{"title": "å¤±çœ çš„å¤œæ™š", "polished_content": "æˆ‘å…ˆè¯•ä¸€ä¸‹è¯­éŸ³è¾“å…¥ï¼Œç°åœ¨æ€ä¹ˆæ ·ã€‚å“å‘€ï¼Œå°±æ˜¯æœ‰ç‚¹å¤±è½ï¼Œå› ä¸ºæ˜æ˜åº”è¯¥æ—©ç‚¹ç¡çš„ã€‚"}}
âŒ WRONG: {{"title": "Reflections on Sleepless Nights"}} - This is English, but user wrote in Chinese!

Example 2 (User writes in English - Title MUST be English):
Input: "today was good i went to park and saw many flowers"
Output: {{"title": "A Day at the Park", "polished_content": "Today was good. I went to the park and saw many flowers."}}
âŒ WRONG: {{"title": "å…¬å›­ä¸€æ—¥"}} - This is Chinese, but user wrote in English!

Example 3 (User writes in Chinese with some English words - Title MUST be Chinese):
Input: "ä»Šå¤©å»äº†parkï¼Œçœ‹åˆ°äº†å¾ˆå¤šflowersï¼Œå¿ƒæƒ…å¾ˆå¥½"
Output: {{"title": "å…¬å›­é‡Œçš„èŠ±", "polished_content": "ä»Šå¤©å»äº†parkï¼Œçœ‹åˆ°äº†å¾ˆå¤šflowersï¼Œå¿ƒæƒ…å¾ˆå¥½ã€‚"}}
âœ… CORRECT: Title is in Chinese because user's primary language is Chinese

Example 4 (User writes in English with some Chinese words - Title MUST be English):
Input: "I went to å…¬å›­ today and saw many èŠ±"
Output: {{"title": "A Visit to the Park", "polished_content": "I went to å…¬å›­ today and saw many èŠ±."}}
âœ… CORRECT: Title is in English because user's primary language is English"""

            # æ„å»ºç”¨æˆ·æ¶ˆæ¯å†…å®¹
            user_content = []
            
            # å¦‚æœæœ‰å›¾ç‰‡ï¼Œæ·»åŠ å›¾ç‰‡åˆ°æ¶ˆæ¯ä¸­ï¼ˆä½¿ç”¨visionèƒ½åŠ›ï¼‰
            if encoded_images and len(encoded_images) > 0:
                print(f"ğŸ–¼ï¸ æ·»åŠ  {len(encoded_images)} å¼ å›¾ç‰‡åˆ° Vision è¯·æ±‚ (Low-res æ¨¡å¼)...")
                for image_data in encoded_images:
                    user_content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_data}",
                            "detail": "low"  # âœ… ä½¿ç”¨ä½åˆ†è¾¨ç‡æ¨¡å¼ï¼Œå¤„ç†æ›´å¿«ä¸”æ›´çœé’±
                        }
                    })
                
                # æ·»åŠ æ–‡å­—å†…å®¹
                user_content.append({
                    "type": "text",
                    "text": f"Please polish this diary entry (preserve ALL content) and create a title. Consider both the images and the text:\n\n{text}"
                })
                user_prompt = user_content
            else:
                # åªæœ‰æ–‡å­—ï¼Œä½¿ç”¨çº¯æ–‡æœ¬
                user_prompt = f"Please polish this diary entry (preserve ALL content):\n\n{text}"
            
            # âœ… åŠ¨æ€è®¡ç®— max_tokensï¼šç¡®ä¿è¶³å¤Ÿè¾“å‡ºå®Œæ•´å†…å®¹
            # åŸå§‹æ–‡æœ¬é•¿åº¦ + æ ‡é¢˜ + JSON æ ¼å¼å¼€é”€ + å®‰å…¨è¾¹è·
            original_length = len(text)
            # å¦‚æœæœ‰å›¾ç‰‡ï¼Œéœ€è¦é¢å¤–çš„tokensï¼ˆæ¯å¼ å›¾ç‰‡çº¦85 tokensï¼‰
            image_tokens = len(encoded_images) * 85 if encoded_images else 0
            # ä¼°ç®—ï¼šåŸå§‹æ–‡æœ¬ * 1.15ï¼ˆ115%é™åˆ¶ï¼‰ + æ ‡é¢˜ï¼ˆ50å­—ç¬¦ï¼‰ + JSONæ ¼å¼ï¼ˆ100å­—ç¬¦ï¼‰ + å®‰å…¨è¾¹è·ï¼ˆ500å­—ç¬¦ï¼‰
            estimated_output_length = int(original_length * 1.15) + 50 + 100 + 500
            # max_tokens å¤§çº¦æ˜¯å­—ç¬¦æ•°çš„ 0.75ï¼ˆä¸­æ–‡ï¼‰åˆ° 1.5ï¼ˆè‹±æ–‡ï¼‰ï¼Œå–ä¸­é—´å€¼ 1.0
            max_tokens = max(2000, int(estimated_output_length * 1.0) + image_tokens)
            # ä½†ä¸è¦è¶…è¿‡ OpenAI çš„é™åˆ¶ï¼ˆGPT-4o-mini æ”¯æŒ 16384 tokensï¼‰
            max_tokens = min(max_tokens, 16000)
            
            print(f"ğŸ“¤ GPT-4o-mini: å‘é€è¯·æ±‚åˆ° OpenAI...")
            print(f"   æ¨¡å‹: {self.MODEL_CONFIG['haiku']}")
            print(f"   åŸå§‹æ–‡æœ¬é•¿åº¦: {original_length} å­—ç¬¦")
            print(f"   å›¾ç‰‡æ•°é‡: {len(encoded_images) if encoded_images else 0}")
            print(f"   ä¼°ç®—è¾“å‡ºé•¿åº¦: {estimated_output_length} å­—ç¬¦")
            print(f"   è®¾ç½® max_tokens: {max_tokens}")
            
            # æ„å»ºæ¶ˆæ¯
            if encoded_images and len(encoded_images) > 0:
                # ä½¿ç”¨visionæ ¼å¼ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            else:
                # çº¯æ–‡æœ¬æ ¼å¼
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            
            # ä½¿ç”¨ OpenAI clientï¼ˆå·²ç»åœ¨ __init__ ä¸­åˆå§‹åŒ–ï¼‰
            response = await asyncio.to_thread(
                self.openai_client.chat.completions.create,
                model=self.MODEL_CONFIG["haiku"],
                messages=messages,
                temperature=0.3,
                max_tokens=max_tokens,
                response_format={"type": "json_object"}  # å¼ºåˆ¶ JSON æ ¼å¼
            )
            
            # è§£æå“åº”
            content = response.choices[0].message.content
            if not content:
                raise ValueError("OpenAI è¿”å›ç©ºå“åº”")
            
            print(f"âœ… GPT-4o-mini: æ”¶åˆ°å“åº”")
            print(f"ğŸ“ GPT-4o-mini: å“åº”å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            
            # è§£æ JSON
            try:
                result = json.loads(content)
                polished_content = result.get("polished_content", text)
                
                # âœ… æ·»åŠ é•¿åº¦å¯¹æ¯”æ—¥å¿—ï¼Œæ£€æŸ¥æ˜¯å¦è¢«æˆªæ–­
                original_length = len(text)
                polished_length = len(polished_content)
                length_ratio = polished_length / original_length if original_length > 0 else 0
                
                print(f"âœ… GPT-4o-mini: æ¶¦è‰²å®Œæˆ")
                print(f"ğŸ“Š é•¿åº¦å¯¹æ¯”: åŸå§‹={original_length} å­—ç¬¦, æ¶¦è‰²å={polished_length} å­—ç¬¦, æ¯”ä¾‹={length_ratio:.2%}")
                
                # âš ï¸ å¦‚æœæ¶¦è‰²åå†…å®¹æ˜æ˜¾å°‘äºåŸå§‹å†…å®¹ï¼ˆå°äº80%ï¼‰ï¼Œå¯èƒ½æ˜¯è¢«æˆªæ–­äº†
                if polished_length < original_length * 0.8:
                    print(f"âš ï¸ è­¦å‘Šï¼šæ¶¦è‰²åå†…å®¹æ˜æ˜¾å°‘äºåŸå§‹å†…å®¹ï¼Œå¯èƒ½è¢«æˆªæ–­ï¼")
                    print(f"   åŸå§‹å†…å®¹å‰100å­—ç¬¦: {text[:100]}...")
                    print(f"   æ¶¦è‰²åå†…å®¹å‰100å­—ç¬¦: {polished_content[:100]}...")
                    # å¦‚æœç¡®å®è¢«æˆªæ–­ï¼Œä½¿ç”¨åŸå§‹å†…å®¹ä½œä¸ºé™çº§æ–¹æ¡ˆ
                    polished_content = text
                    print(f"   ä½¿ç”¨åŸå§‹å†…å®¹ä½œä¸ºé™çº§æ–¹æ¡ˆ")
                
                return {
                    "title": result.get("title", "Today's Reflection"),
                    "polished_content": polished_content
                }
            except json.JSONDecodeError as e:
                print(f"âš ï¸ GPT-4o-mini: JSON è§£æå¤±è´¥: {e}")
                print(f"   åŸå§‹å“åº”: {content[:200]}...")
                # å°è¯•ä»æ–‡æœ¬ä¸­æå– JSON
                import re
                json_match = re.search(r'\{[^{}]*"title"[^{}]*"polished_content"[^{}]*\}', content)
                if json_match:
                    try:
                        result = json.loads(json_match.group())
                        return {
                            "title": result.get("title", "Today's Reflection"),
                            "polished_content": result.get("polished_content", text)
                        }
                    except:
                        pass
                
                # é™çº§æ–¹æ¡ˆ
                print(f"âš ï¸ GPT-4o-mini: ä½¿ç”¨é™çº§æ–¹æ¡ˆ")
                return {
                    "title": "Today's Reflection" if language == "English" else "ä»Šæ—¥è®°å½•",
                    "polished_content": text
                }
        
        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)
            print(f"âŒ GPT-4o-mini è°ƒç”¨å¤±è´¥: {error_type}: {error_msg}")
            
            # è¯¦ç»†é”™è¯¯ä¿¡æ¯
            import traceback
            error_trace = traceback.format_exc()
            print(f"ğŸ“ GPT-4o-mini å®Œæ•´é”™è¯¯å †æ ˆ:")
            print(error_trace)
            
            # æ£€æŸ¥å¸¸è§é”™è¯¯ç±»å‹
            if "RateLimitError" in error_type or "rate_limit" in error_msg.lower():
                print(f"âš ï¸ OpenAI API é™æµ: è¯·æ±‚é¢‘ç‡è¿‡é«˜")
                print(f"ğŸ’¡ å»ºè®®: ç¨åé‡è¯•ï¼Œæˆ–æ£€æŸ¥ OpenAI è´¦æˆ·çš„é…é¢é™åˆ¶")
            elif "AuthenticationError" in error_type or "InvalidApiKey" in error_type:
                print(f"âš ï¸ OpenAI API Key é”™è¯¯: è¯·æ£€æŸ¥ OPENAI_API_KEY ç¯å¢ƒå˜é‡")
            elif "APIConnectionError" in error_type:
                print(f"âš ï¸ OpenAI API è¿æ¥é”™è¯¯: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            
            # é™çº§æ–¹æ¡ˆ
            return {
                "title": "Today's Reflection" if language == "English" else "ä»Šæ—¥è®°å½•",
                "polished_content": text
            }
    
    # ========================================================================
    # ğŸ”¥ GPT-4o-mini è°ƒç”¨ï¼ˆAI åé¦ˆï¼‰
    # ========================================================================
    
    async def _call_gpt4o_mini_for_feedback(
        self, 
        text: str,
        language: str,
        user_name: Optional[str] = None,
        encoded_images: Optional[List[str]] = None
    ) -> str:
        """
        è°ƒç”¨ GPT-4o-mini ç”Ÿæˆæ¸©æš–çš„ AI åé¦ˆ
        
        ğŸ“š å­¦ä¹ ç‚¹ï¼šè¿™ä¸ªå‡½æ•°åŸºäºç”¨æˆ·çš„åŸå§‹æ–‡æœ¬ç”Ÿæˆåé¦ˆ
        - æ›´çœŸå®ï¼šä¿ç•™ç”¨æˆ·æœ€åŸå§‹çš„æƒ…æ„Ÿè¡¨è¾¾
        - æ›´å¿«ï¼šä¸éœ€è¦ç­‰å¾…æ¶¦è‰²å®Œæˆï¼ˆå¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼‰
        - æ›´æ¸©æš–ï¼šAI å›åº”"çœŸå®çš„ä½ "è€Œä¸æ˜¯"å®Œç¾çš„æ–‡å­—"
        
        ä¸ºä»€ä¹ˆé€‰æ‹© GPT-4o-miniï¼Ÿ
        - å…±æƒ…èƒ½åŠ›ç¨³å®š
        - ä¸­è‹±æ–‡è¡¨è¾¾è‡ªç„¶
        - ä¸æ¶¦è‰²æ¨¡å‹ç»Ÿä¸€ï¼Œæ–¹ä¾¿ç»´æŠ¤
        
        è¿”å›:
            æ¸©æš–çš„åé¦ˆæ–‡å­—ï¼ˆç®€æ´æœ‰åŠ›ï¼Œä¸è¶…è¿‡ç”¨æˆ·è¾“å…¥é•¿åº¦ï¼‰
        """
        try:
            print(f"ğŸ’¬ GPT-4o-mini: å¼€å§‹ç”Ÿæˆåé¦ˆï¼ˆåŸºäºåŸå§‹æ–‡æœ¬ï¼‰...")
            print(f"ğŸ‘¤ ç”¨æˆ·åå­—: {user_name if user_name else 'æœªæä¾›'}")
            
            # è®¡ç®—ç”¨æˆ·è¾“å…¥é•¿åº¦ï¼Œç”¨äºåŠ¨æ€è°ƒæ•´åé¦ˆé•¿åº¦
            user_text_length = len(text.strip())
            # åé¦ˆé•¿åº¦ç­–ç•¥ï¼šä¸è¶…è¿‡ç”¨æˆ·è¾“å…¥é•¿åº¦ï¼Œä½†æœ€çŸ­ä¸å°‘äº20å­—ï¼ˆä¸­æ–‡ï¼‰æˆ–15è¯ï¼ˆè‹±æ–‡ï¼‰
            max_feedback_length = max(user_text_length, 20 if language == "Chinese" else 15)
            
            # æ„å»ºä¸ªæ€§åŒ–çš„åå­—ç§°å‘¼
            name_greeting = ""
            if user_name and user_name.strip():
                # æå–åå­—ï¼ˆå»æ‰å¯èƒ½çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ï¼‰
                import re
                first_name = re.split(r'\s+', user_name.strip())[0]
                if language == "Chinese":
                    name_greeting = f"ï¼Œ{first_name}"
                else:
                    name_greeting = f", {first_name}"
            
            # æ„å»ºç»Ÿä¸€çš„ç³»ç»Ÿæç¤ºè¯ (æ”¯æŒè‡ªåŠ¨è¯­è¨€æ£€æµ‹)
            system_prompt = f"""You are a warm, empathetic listener responding to a diary entry.

LANGUAGE RULES:
1. **Detect and Follow**: Detect the user's language from their input (text or voice transcription) and respond in THE SAME LANGUAGE (e.g., if they write in Chinese, respond in Chinese; if Japanese, respond in Japanese).
2. **Fallback**: If the user's input is empty or only contains images, respond in {language}.
3. **Consistency**: NEVER translate. Match the emotional tone and language exactly.

âš ï¸ CRITICAL RULES - YOU MUST FOLLOW:
1. **NEVER ask questions**: Do not ask "How are you?" or "What's on your mind?". No question marks allowed.
2. **Warm Listener**: Your role is to listen and provide emotional resonance, NOT to start a conversation.
3. **Short and Powerful**: 1-2 sentences maximum. Keep it concise.
4. **Greeting**: {"Your response MUST start with '" + user_name + (", " if language == "English" else "ï¼Œ") + "'." if user_name else "Start your response directly."}

Your style:
- Like a close friend, genuine and empathetic.
- Acknowledge their feelings with warmth.
- Natural, conversational, intimate tone.

Response format: Plain text only (NO JSON, NO quotes, NO markdown)."""

            # æ„å»ºä¸ªæ€§åŒ–çš„ç”¨æˆ·æç¤º
            user_content = []
            
            # å¦‚æœæœ‰å›¾ç‰‡ï¼Œæ·»åŠ å›¾ç‰‡åˆ°æ¶ˆæ¯ä¸­ï¼ˆä½¿ç”¨visionèƒ½åŠ›ï¼‰
            if encoded_images and len(encoded_images) > 0:
                print(f"ğŸ–¼ï¸ æ·»åŠ  {len(encoded_images)} å¼ å›¾ç‰‡åˆ° Vision åé¦ˆè¯·æ±‚ (Low-res æ¨¡å¼)...")
                for image_data in encoded_images:
                    user_content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_data}",
                            "detail": "low"
                        }
                    })
                
                # æ·»åŠ æ–‡å­—å†…å®¹
                # æ·»åŠ æ–‡å­—å†…å®¹ (ä½¿ç”¨ä¸­æ€§æŒ‡ä»¤ï¼Œé¿å…è¯­è¨€è¯±å¯¼)
                if text.strip():
                    text_content = f"The user shared this (including images):\n\n{text}\n\nRespond with warmth and empathy in the SAME LANGUAGE as the user's input (NEVER ask questions):"
                else:
                    text_content = f"The user shared some images. Feel the atmosphere and respond with warmth and empathy in {language} (NEVER ask questions):"
                
                user_content.append({
                    "type": "text",
                    "text": text_content
                })
                user_prompt = user_content
            else:
                # åªæœ‰æ–‡å­—ï¼Œä½¿ç”¨çº¯æ–‡æœ¬
                user_prompt = f"The user shared this:\n\n{text}\n\nRespond with warmth and empathy in the SAME LANGUAGE as the user's input (NEVER ask questions):"
            
            # è°ƒç”¨ OpenAI Chat Completions API
            # åŠ¨æ€è°ƒæ•´ max_tokensï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥é•¿åº¦ï¼Œé¢„ç•™æ˜µç§°ä¸æç¤ºç©ºé—´
            estimated_output_length = max_feedback_length + 40
            image_tokens = len(encoded_images) * 85 if encoded_images else 0
            max_tokens = max(200, min(int(estimated_output_length * 1.2) + image_tokens, 800))

            print(f"ğŸ“¤ GPT-4o-mini: å‘é€è¯·æ±‚åˆ° OpenAI...")
            print(f"   æ¨¡å‹: {self.MODEL_CONFIG['sonnet']}")
            print(f"   ç”¨æˆ·åå­—: {user_name if user_name else 'æœªæä¾›'}")
            print(f"   å›¾ç‰‡æ•°é‡: {len(encoded_images) if encoded_images else 0}")
            print(f"   System prompt å‰100å­—ç¬¦: {system_prompt[:100]}...")

            # æ„å»ºæ¶ˆæ¯
            if encoded_images and len(encoded_images) > 0:
                # ä½¿ç”¨visionæ ¼å¼ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            else:
                # çº¯æ–‡æœ¬æ ¼å¼
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]

            response = await asyncio.to_thread(
                self.openai_client.chat.completions.create,
                model=self.MODEL_CONFIG["sonnet"],
                messages=messages,
                temperature=0.7,
                max_tokens=max_tokens,
            )

            content = response.choices[0].message.content if response.choices else ""
            if not content:
                raise ValueError("OpenAI è¿”å›ç©ºå“åº”")

            feedback = content.strip()
            print(f"âœ… GPT-4o-mini: æ”¶åˆ°åé¦ˆï¼Œé•¿åº¦ {len(feedback)} å­—ç¬¦")
            
            if user_name and user_name.strip():
                trimmed_feedback = feedback.lstrip()
                starts_with_name = trimmed_feedback.lower().startswith(user_name.lower())
                
                # æ™ºèƒ½åˆ†éš”ç¬¦ï¼šæ ¹æ®åé¦ˆå†…å®¹åˆ¤æ–­ç”¨ä¸­æ–‡é€—å·è¿˜æ˜¯è‹±æ–‡é€—å·
                # CJK å­—ç¬¦ï¼ˆä¸­æ—¥éŸ©ï¼‰ä½¿ç”¨ä¸­æ–‡é€—å·ï¼Œå…¶ä»–ä½¿ç”¨è‹±æ–‡é€—å·
                import re
                has_cjk = bool(re.search(r'[\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff\uac00-\ud7af]', trimmed_feedback))
                separator = "ï¼Œ" if has_cjk else ", "
                
                if not starts_with_name:
                    print(
                        f"âš ï¸ åé¦ˆæœªä»¥åå­—å¼€å¤´ï¼Œè‡ªåŠ¨ä¿®æ­£: user_name={user_name}, feedback='{feedback}'"
                    )
                    feedback = f"{user_name}{separator}{trimmed_feedback}"
                    print(f"âœ… ä¿®æ­£å: {feedback[:50]}...")
            
            print(f"âœ… GPT-4o-mini: åé¦ˆç”Ÿæˆå®Œæˆ")
            print(f"   åé¦ˆ: {feedback[:50]}...")
            
            return feedback
        
        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)
            print(f"âŒ GPT-4o-mini åé¦ˆè°ƒç”¨å¤±è´¥: {error_type}: {error_msg}")
            
            # è¯¦ç»†é”™è¯¯ä¿¡æ¯
            import traceback
            error_trace = traceback.format_exc()
            print(f"ğŸ“ GPT-4o-mini åé¦ˆå®Œæ•´é”™è¯¯å †æ ˆ:")
            print(error_trace)
            
            # æ£€æŸ¥å¸¸è§é”™è¯¯ç±»å‹
            if "RateLimit" in error_type or "rate limit" in error_msg.lower():
                print(f"âš ï¸ OpenAI é™æµ: è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œå»ºè®®ç¨åé‡è¯•æˆ–è°ƒæ•´é€Ÿç‡")
            elif "AuthenticationError" in error_type or "InvalidApiKey" in error_type:
                print(f"âš ï¸ OpenAI API Key é”™è¯¯: è¯·æ£€æŸ¥ OPENAI_API_KEY ç¯å¢ƒå˜é‡")
            elif "APIConnectionError" in error_type:
                print(f"âš ï¸ OpenAI API è¿æ¥é”™è¯¯: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            
            # é™çº§æ–¹æ¡ˆ
            return "æ„Ÿè°¢åˆ†äº«ä½ çš„è¿™ä¸€åˆ»ã€‚" if language == "Chinese" else "Thanks for sharing this moment."
    
    # ========================================================================
    # éªŒè¯å’Œé™çº§é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
    # ========================================================================
    
    def _validate_and_fix_result(
        self, 
        result: Dict[str, str], 
        original_text: str
    ) -> Dict[str, str]:
        """
        éªŒè¯å¹¶ä¿®æ­£AIè¾“å‡º - è´¨é‡æŠŠå…³
        
        ğŸ”¥ æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•å®Œå…¨ä¿æŒä¸å˜
        """
        import re
        
        orig_len = len(original_text.strip())
        
        # æ£€æµ‹è¯­è¨€
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', original_text))
        is_chinese = chinese_chars > len(original_text) * 0.2
        
        print(f"ğŸ“Š åŸæ–‡è¯­è¨€æ£€æµ‹: æ€»é•¿åº¦={len(original_text)}, ä¸­æ–‡å­—ç¬¦={chinese_chars}, åˆ¤å®š={'ä¸­æ–‡' if is_chinese else 'è‹±æ–‡'}")
        
        # æå–å„éƒ¨åˆ†
        title = (result.get("title", "") or "").strip()
        polished = (result.get("polished_content", "") or "").strip()
        feedback = (result.get("feedback", "") or "").strip()
        
        # ğŸ”¥ å¼ºåŒ–è¯­è¨€ä¸€è‡´æ€§éªŒè¯ï¼šæ›´å‡†ç¡®åœ°æ£€æµ‹å’Œä¿®æ­£
        title_has_chinese = bool(re.search(r'[\u4e00-\u9fff]', title))
        title_has_english = bool(re.search(r'[a-zA-Z]', title))
        feedback_has_chinese = bool(re.search(r'[\u4e00-\u9fff]', feedback))
        
        used_fallback = False
        
        # ğŸ”¥ æ›´ä¸¥æ ¼çš„æ ‡é¢˜è¯­è¨€æ£€æŸ¥
        # å¦‚æœç”¨æˆ·è¾“å…¥æ˜¯ä¸­æ–‡ï¼Œä½†æ ‡é¢˜åŒ…å«è‹±æ–‡ä¸”æ²¡æœ‰ä¸­æ–‡ï¼Œåˆ¤å®šä¸ºä¸ä¸€è‡´
        # å¦‚æœç”¨æˆ·è¾“å…¥æ˜¯è‹±æ–‡ï¼Œä½†æ ‡é¢˜åŒ…å«ä¸­æ–‡ä¸”æ²¡æœ‰è‹±æ–‡ï¼Œåˆ¤å®šä¸ºä¸ä¸€è‡´
        title_language_mismatch = False
        if is_chinese:
            # ç”¨æˆ·è¾“å…¥æ˜¯ä¸­æ–‡ï¼Œæ ‡é¢˜åº”è¯¥æ˜¯ä¸­æ–‡
            if not title_has_chinese and title_has_english:
                title_language_mismatch = True
                print(f"âš ï¸ æ ‡é¢˜è¯­è¨€ä¸ä¸€è‡´ï¼ç”¨æˆ·è¾“å…¥æ˜¯ä¸­æ–‡ï¼Œä½†æ ‡é¢˜æ˜¯è‹±æ–‡: '{title}'")
        else:
            # ç”¨æˆ·è¾“å…¥æ˜¯è‹±æ–‡ï¼Œæ ‡é¢˜åº”è¯¥æ˜¯è‹±æ–‡
            if not title_has_english and title_has_chinese:
                title_language_mismatch = True
                print(f"âš ï¸ æ ‡é¢˜è¯­è¨€ä¸ä¸€è‡´ï¼ç”¨æˆ·è¾“å…¥æ˜¯è‹±æ–‡ï¼Œä½†æ ‡é¢˜æ˜¯ä¸­æ–‡: '{title}'")
        
        if title_language_mismatch:
            # ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼Œç¡®ä¿è¯­è¨€ä¸€è‡´
            title = "ä»Šæ—¥è®°å½•" if is_chinese else "Today's Reflection"
            used_fallback = True
            print(f"âœ… å·²ä¿®æ­£æ ‡é¢˜ä¸º: '{title}'")
        
        if is_chinese != feedback_has_chinese:
            print(f"âš ï¸ åé¦ˆè¯­è¨€ä¸ä¸€è‡´ï¼")
            feedback = "æ„Ÿè°¢åˆ†äº«ä½ çš„è¿™ä¸€åˆ»ã€‚" if is_chinese else "Thanks for sharing this moment."
            used_fallback = True
        
        # æ¸…ç†å‡½æ•°
        def clean_text(text: str) -> str:
            text = re.sub(r'[\U0001F300-\U0001FAFF\U00002700-\U000027BF]+', '', text)
            text = text.replace('ï¼', 'ã€‚').replace('!', '.')
            text = re.sub(r'\s+', ' ', text).strip()
            return text

        def clean_text_preserve_formatting(
            text: str,
            is_chinese: bool,
            should_adjust_paragraphs: bool,
        ) -> str:
            """
            ä¿ç•™ç”¨æˆ·æ’ç‰ˆï¼ˆæ¢è¡Œ/åˆ—è¡¨ï¼‰ï¼Œåªåšè½»åº¦æ¸…ç†ã€‚
            """
            text = re.sub(r'[\U0001F300-\U0001FAFF\U00002700-\U000027BF]+', '', text)
            text = text.replace('ï¼', 'ã€‚').replace('!', '.')
            text = text.replace('\r\n', '\n').replace('\r', '\n')
            lines = text.split('\n')
            cleaned_lines = []
            bullet_pattern = re.compile(r'^(\s*)([-*â€¢]|\d+[.)])\s*(.*)$')
            for line in lines:
                if not line.strip():
                    cleaned_lines.append("")
                    continue
                bullet_match = bullet_pattern.match(line)
                if bullet_match:
                    indent, marker, content = bullet_match.groups()
                    content = re.sub(r'\s+', ' ', content).strip()
                    cleaned_lines.append(f"{indent}{marker} {content}".rstrip())
                else:
                    leading_ws = re.match(r'^\s*', line).group(0)
                    content = line[len(leading_ws):]
                    content = re.sub(r'\s+', ' ', content).strip()
                    cleaned_lines.append(f"{leading_ws}{content}".rstrip())
            cleaned = "\n".join(cleaned_lines).strip()

            if not should_adjust_paragraphs:
                return cleaned

            # å¦‚æœåŒ…å«åˆ—è¡¨ï¼Œé¿å…è‡ªåŠ¨åˆå¹¶æ®µè½
            for line in cleaned.split("\n"):
                if bullet_pattern.match(line):
                    return cleaned

            paragraphs = re.split(r"\n\s*\n+", cleaned)
            if len(paragraphs) <= 1:
                return cleaned

            def sentence_count(paragraph: str) -> int:
                if is_chinese:
                    matches = re.findall(r"[ã€‚ï¼ï¼Ÿ!?ï¼›;]", paragraph)
                else:
                    matches = re.findall(r"[.!?;]", paragraph)
                return max(1, len(matches))

            def merge_text(a: str, b: str) -> str:
                if not a:
                    return b
                if is_chinese:
                    sep = ""
                else:
                    sep = "" if a.endswith((" ", "\n")) else " "
                return f"{a.rstrip()}{sep}{b.lstrip()}"

            # âœ… é¦–æ®µè‡³å°‘åŒ…å«3å¥ï¼Œé¿å…ç¬¬ä¸€å¥åæ–­æ®µ
            while len(paragraphs) > 1 and sentence_count(paragraphs[0]) < 3:
                paragraphs[0] = merge_text(paragraphs[0], paragraphs[1])
                paragraphs.pop(1)

            # âœ… åˆå¹¶è¿‡çŸ­æ®µè½ï¼ˆé¿å…ç©ºç™½æ„Ÿï¼‰
            min_chars = 60 if is_chinese else 90
            i = 1
            while i < len(paragraphs):
                if sentence_count(paragraphs[i]) < 2 or len(paragraphs[i]) < min_chars:
                    paragraphs[i - 1] = merge_text(paragraphs[i - 1], paragraphs[i])
                    paragraphs.pop(i)
                else:
                    i += 1

            return "\n\n".join(p.strip() for p in paragraphs)
        
        def trim_to_complete_sentences(text: str, max_len: int) -> str:
            if len(text) <= max_len:
                return text

            sentence_pattern = r"([ã€‚ï¼ï¼Ÿ.!?])(['\"\"ã€ã€)]?)\s*"
            last_end = None

            for match in re.finditer(sentence_pattern, text):
                end_pos = match.end()
                if end_pos <= max_len:
                    last_end = end_pos
                else:
                    break

            if last_end is not None:
                return text[:last_end].rstrip()

            for punct in ['ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?', 'ï¼›', ';']:
                idx = text.rfind(punct, 0, max_len + 1)
                if idx > max_len * 0.5:
                    return text[:idx + 1].rstrip()
            return text[:max_len].rstrip()
        
        # ä¿®æ­£æ ‡é¢˜
        title = clean_text(title)
        title = re.sub(r'[^\w\u4e00-\u9fff\s-]', '', title)
        title = re.sub(r'\s+', ' ', title).strip()
        
        if len(title) < self.LENGTH_LIMITS["title_min"]:
            title = "Today's Reflection" if any(ord(c) < 128 for c in original_text) else "ä»Šæ—¥è®°å½•"
        elif len(title) > self.LENGTH_LIMITS["title_max"]:
            max_len = self.LENGTH_LIMITS["title_max"]
            if ' ' in title and len(title) > max_len:
                words = title[:max_len].rsplit(' ', 1)
                title = words[0] if len(words[0]) > max_len * 0.6 else title[:max_len]
            else:
                title = title[:max_len]
        
        # ä¿®æ­£æ¶¦è‰²å†…å®¹
        original_has_linebreaks = "\n" in original_text
        original_has_list = bool(re.search(r"(?m)^\s*([-*â€¢]|\d+[.)])\s+", original_text))
        should_adjust_paragraphs = not original_has_linebreaks and not original_has_list
        polished = clean_text_preserve_formatting(
            polished,
            is_chinese,
            should_adjust_paragraphs,
        )
        max_polished_len = int(orig_len * self.LENGTH_LIMITS["polished_ratio"])
        
        # âœ… æ·»åŠ é•¿åº¦æ£€æŸ¥æ—¥å¿—
        print(f"ğŸ“Š æ¶¦è‰²å†…å®¹éªŒè¯: åŸå§‹é•¿åº¦={orig_len}, æ¶¦è‰²åé•¿åº¦={len(polished)}, æœ€å¤§å…è®¸é•¿åº¦={max_polished_len}")
        
        # âš ï¸ å¦‚æœæ¶¦è‰²åå†…å®¹æ˜æ˜¾å°‘äºåŸå§‹å†…å®¹ï¼ˆå°äº80%ï¼‰ï¼Œå¯èƒ½æ˜¯è¢«æˆªæ–­äº†ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
        if len(polished) < orig_len * 0.8:
            print(f"âš ï¸ è­¦å‘Šï¼šæ¶¦è‰²åå†…å®¹æ˜æ˜¾å°‘äºåŸå§‹å†…å®¹ï¼ˆ{len(polished)} < {orig_len * 0.8}ï¼‰ï¼Œä½¿ç”¨åŸå§‹å†…å®¹")
            polished = original_text.strip()
        
        # åªæœ‰åœ¨è¶…è¿‡æœ€å¤§é•¿åº¦æ—¶æ‰æˆªæ–­ï¼ˆä½†è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºæç¤ºè¯è¦æ±‚â‰¤115%ï¼‰
        if len(polished) > max_polished_len:
            print(f"âš ï¸ æ¶¦è‰²åå†…å®¹è¶…è¿‡æœ€å¤§é•¿åº¦ï¼ˆ{len(polished)} > {max_polished_len}ï¼‰ï¼ŒæŒ‰å®Œæ•´å¥å­æˆªæ–­")
            polished = trim_to_complete_sentences(polished, max_polished_len)
        
        # ä¿®æ­£åé¦ˆ
        feedback = clean_text(feedback)
        
        if not used_fallback and len(feedback) < self.LENGTH_LIMITS.get("feedback_min", 20):
            print(f"âš ï¸ åé¦ˆè¿‡çŸ­ï¼Œä½¿ç”¨é™çº§")
            feedback = "æ„Ÿè°¢åˆ†äº«ä½ çš„è¿™ä¸€åˆ»ã€‚" if is_chinese else "Thanks for sharing this moment."
        
        if len(feedback) > self.LENGTH_LIMITS["feedback_max"]:
            print(f"ğŸ“ åé¦ˆè¿‡é•¿ï¼ŒæŒ‰å®Œæ•´å¥å­æˆªæ–­")
            feedback = trim_to_complete_sentences(feedback, self.LENGTH_LIMITS["feedback_max"])
        
        is_english = any(ord(c) < 128 for c in original_text[:50])
        default_feedback = "Thank you for sharing." if is_english else "æ„Ÿè°¢åˆ†äº«ã€‚"
        
        return {
            "title": title,
            "polished_content": polished or original_text,
            "feedback": feedback or default_feedback
        }
    
    def _create_fallback_result(self, text: str) -> Dict[str, str]:
        """
        åˆ›å»ºé™çº§ç»“æœ
        
        ğŸ”¥ æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•å®Œå…¨ä¿æŒä¸å˜
        """
        import re
        
        print("âš ï¸ ä½¿ç”¨é™çº§æ–¹æ¡ˆ")
        
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        is_chinese = chinese_chars > len(text) * 0.2
        
        return {
            "title": "ä»Šæ—¥è®°å½•" if is_chinese else "Today's Reflection",
            "polished_content": text,
            "feedback": "æ„Ÿè°¢åˆ†äº«ã€‚" if is_chinese else "Thanks for sharing."
        }
    
    # ========================================================================
    # å‘åå…¼å®¹æ–¹æ³•ï¼ˆä¿æŒä¸å˜ï¼‰
    # ========================================================================
    
    def polish_text(self, text: str) -> str:
        """æ¶¦è‰²æ–‡æœ¬ï¼ˆæ—§APIï¼‰"""
        result = self.polish_content_multilingual(text)
        return result["polished_content"]
    
    def generate_feedback(self, diary_content: str) -> str:
        """ç”Ÿæˆåé¦ˆï¼ˆæ—§APIï¼‰"""
        result = self.polish_content_multilingual(diary_content)
        return result["feedback"]


    # ========================================================================
    # ğŸ”¥ å›¾ç‰‡ä¸‹è½½å’Œç¼–ç ï¼ˆç”¨äºVision APIï¼‰
    # ========================================================================
    
    async def _download_and_encode_image(self, image_url: str) -> str:
        """
        ä¸‹è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64ç¼–ç ï¼ˆç”¨äºOpenAI Vision APIï¼‰
        
        Args:
            image_url: å›¾ç‰‡çš„URLï¼ˆS3 URLæˆ–HTTP URLï¼‰
        
        Returns:
            base64ç¼–ç çš„å›¾ç‰‡æ•°æ®
        """
        try:
            print(f"ğŸ“¥ ä¸‹è½½å›¾ç‰‡: {image_url[:50]}...")
            
            # ä¸‹è½½å›¾ç‰‡
            response = await asyncio.to_thread(requests.get, image_url, timeout=10)
            response.raise_for_status()
            
            # è½¬æ¢ä¸ºbase64
            image_base64 = base64.b64encode(response.content).decode('utf-8')
            
            print(f"âœ… å›¾ç‰‡ä¸‹è½½å¹¶ç¼–ç å®Œæˆï¼Œå¤§å°: {len(image_base64)} å­—ç¬¦")
            return image_base64
            
        except Exception as e:
            print(f"âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
            raise

# ğŸ¯ ä½¿ç”¨ç¤ºä¾‹
"""
# 1. åˆå§‹åŒ–æœåŠ¡
service = OpenAIService()

# 2. è¯­éŸ³è½¬æ–‡å­—ï¼ˆWhisperï¼‰
text = await service.transcribe_audio(audio_bytes, "recording.m4a")

# 3. å¹¶è¡Œå¤„ç†ï¼šæ¶¦è‰²ï¼ˆhaiku å­—æ®µï¼‰+ åé¦ˆï¼ˆsonnet å­—æ®µï¼‰
result = await service.polish_content_multilingual(text)

# 4. ä½¿ç”¨ç»“æœ
print(f"æ ‡é¢˜: {result['title']}")        # GPT-4o-miniï¼ˆhaiku å­—æ®µï¼‰ç”Ÿæˆ
print(f"å†…å®¹: {result['polished_content']}")  # GPT-4o-miniï¼ˆhaiku å­—æ®µï¼‰æ¶¦è‰²
print(f"åé¦ˆ: {result['feedback']}")      # GPT-4o-miniï¼ˆsonnet å­—æ®µï¼‰ç”Ÿæˆ

# 5. å›¾ç‰‡+æ–‡å­—å¤„ç†ï¼ˆæ–°åŠŸèƒ½ï¼‰
result = await service.polish_content_multilingual(
    text="ä»Šå¤©å»äº†å…¬å›­",
    image_urls=["https://s3.../image1.jpg", "https://s3.../image2.jpg"]
)
"""
