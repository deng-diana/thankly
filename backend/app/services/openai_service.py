"""
AI æœåŠ¡ - æ··åˆæ¨¡å‹ä¼˜åŒ–ç‰ˆæœ¬
ä½œè€…çµæ„Ÿæ¥æºï¼šä¹”å¸ƒæ–¯çš„ç®€çº¦å“²å­¦ + å¼ å°é¾™çš„å…‹åˆ¶è®¾è®¡

ğŸ”¥ æœ€æ–°æ›´æ–°ï¼š
1. AI æš–å¿ƒåé¦ˆä» Claude Sonnet å›å½’ OpenAI GPT-4o-miniï¼ˆTestFlight éªŒè¯ç¨³å®šç‰ˆï¼‰
2. æ¶¦è‰² + æ ‡é¢˜ä¸åé¦ˆç»Ÿä¸€ä½¿ç”¨ GPT-4o-miniï¼ˆé™ä½ç»´æŠ¤æˆæœ¬ï¼‰
3. å¹¶è¡Œæ‰§è¡Œç­–ç•¥ä¿æŒä¸å˜ï¼Œæ€§èƒ½ç»§ç»­ç¨³å®š
4. Whisper è¯­éŸ³è½¬æ–‡å­—æŒç»­æ²¿ç”¨ï¼Œä¿è¯è¯†åˆ«å‡†ç¡®åº¦

æ ¸å¿ƒç†å¿µï¼š
1. ç®€å•ä½†ä¸ç®€é™‹ï¼ˆSimple but not simplisticï¼‰
2. å¼ºå¤§ä½†ä¸å¤æ‚ï¼ˆPowerful but not complicatedï¼‰
3. ä¼˜é›…ä½†ä¸ç‚«æŠ€ï¼ˆElegant but not showyï¼‰
"""

import tempfile
import os
import json
import asyncio  # ğŸ”¥ ç”¨äºå¹¶è¡Œæ‰§è¡Œ
from typing import Dict, Optional, List, Any
from openai import OpenAI
import io
import base64
import requests

from ..config import get_settings


class OpenAIService:
    """
    AI æœåŠ¡ç±» - æ”¯æŒå¤šè¯­è¨€æ—¥è®°å¤„ç†
    
    è¿™ä¸ªç±»å°±åƒä¸€ä¸ªæ¸©æŸ”çš„æ—¥è®°åŠ©æ‰‹ï¼Œå®ƒä¼šï¼š
    1. å¬æ‡‚ä½ çš„å£°éŸ³ï¼ˆè¯­éŸ³è½¬æ–‡å­— - Whisperï¼‰
    2. ç¾åŒ–ä½ çš„æ–‡å­—ï¼ˆè½»åº¦æ¶¦è‰² - GPT-4o-miniï¼‰
    3. ç»™ä½ æ¸©æš–çš„å›åº”ï¼ˆå¿ƒç†é™ªä¼´ - GPT-4o-miniï¼‰
    4. å¸®ä½ èµ·ä¸ªå¥½æ ‡é¢˜ï¼ˆç”»é¾™ç‚¹ç› - GPT-4o-miniï¼‰
    
    ğŸ”¥ æ¨¡å‹é€‰æ‹©ç­–ç•¥ï¼š
    - Whisper: è¯­éŸ³è½¬æ–‡å­—ï¼ˆOpenAIï¼Œæ— å¯æ›¿ä»£ï¼‰
    - GPT-4o-mini: æ¶¦è‰² + æ ‡é¢˜ï¼ˆå¿«é€Ÿã€ç¨³å®šã€æˆæœ¬å¯æ§ï¼‰
    - GPT-4o-mini: AI åé¦ˆï¼ˆTestFlight å›å½’éªŒè¯æ›´ç¨³å®šï¼‰
    """
    
    # ğŸ¯ æ¨¡å‹é…ç½®
    MODEL_CONFIG = {
        # è¯­éŸ³è½¬æ–‡å­—ï¼ˆä¿æŒä¸å˜ï¼‰
        "transcription": "whisper-1",
        
        # ğŸ”¥ GPT æ¨¡å‹é…ç½®
        "haiku": "gpt-4o-mini",  # æ¶¦è‰² + æ ‡é¢˜ï¼ˆå‘½åæ²¿ç”¨æ—§å­—æ®µï¼Œä¾¿äºå…¼å®¹ï¼‰
        "sonnet": "gpt-4o-mini",  # AI æš–å¿ƒåé¦ˆï¼ˆå›å½’ OpenAI æ¨¡å‹ï¼‰
        
        # ğŸ¤ ä¸ºä»€ä¹ˆ Whisperï¼Ÿ
        # âœ… OpenAI å®˜æ–¹è¯­éŸ³è½¬æ–‡å­—æ¨¡å‹
        # âœ… æ”¯æŒ 100+ è¯­è¨€ï¼ˆä¸­è‹±æ–‡å®Œç¾ï¼‰
        # âœ… é«˜å‡†ç¡®åº¦ï¼Œä½å¹»è§‰ç‡
        
        # ğŸ¨ ä¸ºä»€ä¹ˆ Haiku æ¶¦è‰²ï¼Ÿ
        # âœ… é€Ÿåº¦å¿«ï¼ˆ1-2ç§’ï¼‰
        # âœ… ä¾¿å®œï¼ˆ$1/1M tokens inputï¼‰
        # âœ… è¶³å¤Ÿèªæ˜ï¼ˆæ—¥è®°æ¶¦è‰²ç»°ç»°æœ‰ä½™ï¼‰
        
        # ğŸ’¬ ä¸ºä»€ä¹ˆ GPT-4o-mini åé¦ˆï¼Ÿ
        # âœ… æ¸©æš–çœŸå®ï¼ˆå…¼é¡¾å…±æƒ…ä¸å®‰å…¨ï¼‰
        # âœ… å¤šè¯­è¨€èƒ½åŠ›å¼ºï¼ˆä¸­è‹±æ–‡éƒ½è‡ªç„¶ï¼‰
        # âœ… ä¸æ¶¦è‰²æ¨¡å‹ç»Ÿä¸€ï¼Œæ–¹ä¾¿ç»´æŠ¤
    }
    
    # ğŸ“ é•¿åº¦é™åˆ¶ï¼ˆä¿æŒä¸å˜ï¼‰
    LENGTH_LIMITS = {
        "title_min": 4,
        "title_max": 50,
        "feedback_min": 30,
        "feedback_max": 250,
        "polished_ratio": 1.15,
        "min_audio_text": 5,
    }
    
    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡å®¢æˆ·ç«¯"""
        settings = get_settings()
        
        # OpenAI å®¢æˆ·ç«¯ï¼ˆç”¨äº Whisperï¼‰
        self.openai_client = OpenAI(api_key=settings.openai_api_key)
        self.openai_api_key = settings.openai_api_key
        
        print(f"âœ… AI æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        print(f"   - Whisper: è¯­éŸ³è½¬æ–‡å­—")
        print(f"   - GPT-4o-mini: æ¶¦è‰² + æ ‡é¢˜ (é…ç½®å­—æ®µ haiku)")
        print(f"   - GPT-4o-mini: AI åé¦ˆ (é…ç½®å­—æ®µ sonnet)")
    
    # ========================================================================
    # è¯­éŸ³è½¬æ–‡å­—ï¼ˆä¿æŒä¸å˜ï¼‰
    # ========================================================================
    
    async def transcribe_audio(
        self, 
        audio_content: bytes, 
        filename: str,
        expected_duration: Optional[int] = None
    ) -> str:
        """
        è¯­éŸ³è½¬æ–‡å­— - æŠŠä½ çš„å£°éŸ³å˜æˆæ–‡å­—
        
        ğŸ”¥ æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•å®Œå…¨ä¸å˜ï¼Œç»§ç»­ä½¿ç”¨ Whisper
        
        å·¥ä½œæµç¨‹ï¼š
        1. æ”¶åˆ°éŸ³é¢‘ â†’ æ£€æŸ¥å¤§å°
        2. åˆ›å»ºä¸´æ—¶æ–‡ä»¶ â†’ ç¡®ä¿æ ¼å¼æ­£ç¡®
        3. å‘é€ç»™ Whisper â†’ å®ƒæ˜¯è¯­éŸ³è¯†åˆ«ä¸“å®¶
        4. æ£€æŸ¥ç»“æœ â†’ ç¡®ä¿ä¸æ˜¯ç©ºçš„
        5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶ â†’ ä¿æŒæ•´æ´
        """
        temp_file_path = None
        
        try:
            # æ£€æŸ¥éŸ³é¢‘å¤§å°
            audio_size_kb = len(audio_content) / 1024
            print(f"ğŸ¤ æ”¶åˆ°éŸ³é¢‘: {filename}, å¤§å°: {audio_size_kb:.1f} KB")
            
            if audio_size_kb < 1:
                raise ValueError("éŸ³é¢‘æ–‡ä»¶å¤ªå°ï¼Œè¯·è¯´é•¿ä¸€ç‚¹")
            
            # å‡†å¤‡ä¸´æ—¶æ–‡ä»¶
            suffix = '.m4a' if not filename.endswith('.m4a') else ''
            with tempfile.NamedTemporaryFile(
                delete=False, 
                suffix=suffix or os.path.splitext(filename)[1]
            ) as temp_file:
                temp_file.write(audio_content)
                temp_file_path = temp_file.name
            
            print(f"âœ… ä¸´æ—¶æ–‡ä»¶å‡†å¤‡å®Œæˆ")
            
            # è°ƒç”¨ Whisper
            import httpx
            import io
            print("ğŸ“¤ æ­£åœ¨è¯†åˆ«è¯­éŸ³ï¼ˆverbose_json æ¨¡å¼ï¼‰...")
            response_json = None
            try:
                with httpx.Client(timeout=60.0) as client:
                    file_stream = io.BytesIO(audio_content)
                    response = client.post(
                        "https://api.openai.com/v1/audio/transcriptions",
                        headers={
                            "Authorization": f"Bearer {self.openai_api_key}",
                        },
                        data={
                            "model": self.MODEL_CONFIG["transcription"],
                            "language": "",
                            "temperature": "0",
                            "response_format": "verbose_json",
                        },
                        files={
                            "file": (filename or "recording.m4a", file_stream, "audio/m4a"),
                        },
                    )
                    response.raise_for_status()
                    response_json = response.json()
            except httpx.HTTPError as http_err:
                print(f"âŒ Whisper HTTP è¯·æ±‚å¤±è´¥: {http_err}")
                if http_err.response is not None:
                    print(f"ğŸ“„ Whisper å“åº”: {http_err.response.text[:200]}...")
                raise ValueError("è¯­éŸ³è¯†åˆ«å¤±è´¥: æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•")
            
            if not response_json:
                raise ValueError("è¯­éŸ³è¯†åˆ«å¤±è´¥: æœªæ”¶åˆ°æœ‰æ•ˆå“åº”")
            
            text = (response_json.get("text") or "").strip()
            segments = response_json.get("segments", []) or []
            detected_language = response_json.get("language", "").lower()  # âœ… è·å–æ£€æµ‹åˆ°çš„è¯­è¨€
            
            # ğŸ”¥ æ–°å¢ï¼šè¯­è¨€ç™½åå•æ£€æŸ¥ - é˜²æ­¢èƒŒæ™¯éŸ³ä¹è¢«è¯¯è¯†åˆ«ä¸ºéŸ©è¯­/æ—¥è¯­ç­‰
            SUPPORTED_LANGUAGES = {"zh", "en", "chinese", "english"}
            if detected_language and detected_language not in SUPPORTED_LANGUAGES:
                print(f"âŒ æ£€æµ‹åˆ°ä¸æ”¯æŒçš„è¯­è¨€: '{detected_language}'")
                print(f"   è¯†åˆ«æ–‡æœ¬: '{text[:100]}'")
                print(f"   è¿™å¯èƒ½æ˜¯èƒŒæ™¯éŸ³ä¹æˆ–å™ªéŸ³è¢«è¯¯è¯†åˆ«")
                raise ValueError("æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè¯·ç”¨ä¸­æ–‡æˆ–è‹±æ–‡è¯´è¯")
            
            # ğŸ”¥ æ–°å¢ï¼šæ£€æµ‹éŸ©è¯­/æ—¥è¯­å­—ç¬¦ - åŒé‡ä¿é™©
            import re
            korean_chars = len(re.findall(r'[\uac00-\ud7af]', text))  # éŸ©è¯­å­—ç¬¦
            japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', text))  # æ—¥è¯­å­—ç¬¦
            if korean_chars > 3 or japanese_chars > 3:
                print(f"âŒ æ£€æµ‹åˆ°éŸ©è¯­/æ—¥è¯­å­—ç¬¦: éŸ©è¯­={korean_chars}, æ—¥è¯­={japanese_chars}")
                print(f"   è¯†åˆ«æ–‡æœ¬: '{text[:100]}'")
                print(f"   è¿™å¯èƒ½æ˜¯èƒŒæ™¯éŸ³ä¹æˆ–å™ªéŸ³è¢«è¯¯è¯†åˆ«")
                raise ValueError("æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè¯·ç”¨ä¸­æ–‡æˆ–è‹±æ–‡è¯´è¯")
            
            # ğŸ”¥ æ–°å¢ï¼šæ£€æµ‹é‡å¤æ–‡æœ¬æ¨¡å¼ - Whisper å¹»è§‰çš„å¸¸è§ç‰¹å¾
            # ä¾‹å¦‚: "ë‹­ê°€ìŠ´ì‚´ ì¹˜í‚¨ì…ë‹ˆë‹¤. ë‹­ê°€ìŠ´ì‚´ ì¹˜í‚¨ê³¼ ë‹­ê°€ìŠ´ì‚´ ì¹˜í‚¨ì€..."
            words = text.split()
            if len(words) >= 5:
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤§é‡é‡å¤çš„è¯
                word_counts = {}
                for word in words:
                    if len(word) >= 3:  # åªç»Ÿè®¡é•¿åº¦>=3çš„è¯
                        word_counts[word] = word_counts.get(word, 0) + 1
                
                # å¦‚æœæŸä¸ªè¯å‡ºç°æ¬¡æ•°è¶…è¿‡æ€»è¯æ•°çš„40%,å¯èƒ½æ˜¯å¹»è§‰
                max_repetition = max(word_counts.values()) if word_counts else 0
                repetition_ratio = max_repetition / len(words) if len(words) > 0 else 0
                
                if repetition_ratio > 0.4:
                    print(f"âŒ æ£€æµ‹åˆ°é«˜åº¦é‡å¤çš„æ–‡æœ¬æ¨¡å¼: é‡å¤ç‡={repetition_ratio:.1%}")
                    print(f"   è¯†åˆ«æ–‡æœ¬: '{text[:100]}'")
                    print(f"   è¿™å¯èƒ½æ˜¯èƒŒæ™¯éŸ³ä¹æˆ–å™ªéŸ³è¢«è¯¯è¯†åˆ«")
                    raise ValueError("æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè¯·è¯´æ¸…æ¥šä¸€äº›")
            
            normalized_text = re.sub(r"\s+", "", text)
            
            if len(normalized_text) < self.LENGTH_LIMITS["min_audio_text"]:
                print(f"âŒ è½¬å½•å†…å®¹è¿‡çŸ­: '{text}'")
                raise ValueError("æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè¯·è¯´æ¸…æ¥šä¸€äº›")
            
            filler_tokens = {
                "um",
                "uh",
                "uhh",
                "hmm",
                "hmmm",
                "erm",
                "er",
                "ah",
                "oh",
                "mmm",
            }
            token_pattern = r"[A-Za-z\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff]+"
            tokens = re.findall(token_pattern, text)
            meaningful_tokens = [
                token
                for token in tokens
                if len(token) >= 2 and token.lower() not in filler_tokens
            ]
            cjk_chars = re.findall(r"[\u4e00-\u9fff]", text)
            has_cjk = len(cjk_chars) > 0
            
            unique_chars = len(set(normalized_text))
            if unique_chars <= 2 and len(normalized_text) > 2:
                print(
                    "âŒ è½¬å½•ç»“æœåŒ…å«å¤§é‡é‡å¤å­—ç¬¦ï¼Œè§†ä¸ºæ— æ•ˆ:",
                    {"text": text, "normalized": normalized_text},
                )
                raise ValueError("æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè¯·è¯´æ¸…æ¥šä¸€äº›")
            
            # åˆ†æ Whisper æ®µç»“æœï¼Œç¡®è®¤æ˜¯å¦çœŸçš„æœ‰è®²è¯
            def _segment_value(segment, attr, default):
                if isinstance(segment, dict):
                    return segment.get(attr, default)
                return getattr(segment, attr, default)
            
            confident_segments = []
            total_confident_duration = 0.0
            total_segment_duration = 0.0
            avg_no_speech_sum = 0.0
            
            for segment in segments:
                try:
                    start = float(_segment_value(segment, "start", 0))
                    end = float(_segment_value(segment, "end", 0))
                    seg_duration = max(0.0, end - start)
                except (TypeError, ValueError):
                    seg_duration = 0.0
                    start = 0.0
                    end = 0.0
                
                total_segment_duration += seg_duration
                
                try:
                    no_speech_prob = float(_segment_value(segment, "no_speech_prob", 1))
                except (TypeError, ValueError):
                    no_speech_prob = 1
                
                try:
                    avg_logprob = float(_segment_value(segment, "avg_logprob", -10))
                except (TypeError, ValueError):
                    avg_logprob = -10
                
                avg_no_speech_sum += no_speech_prob * seg_duration
                
                if (
                    seg_duration >= 0.3
                    and no_speech_prob < 0.45
                    and avg_logprob > -0.75
                ):
                    confident_segments.append(segment)
                    total_confident_duration += seg_duration
            
            reference_duration = None
            if expected_duration and expected_duration > 0:
                reference_duration = float(expected_duration)
            elif total_segment_duration > 0:
                reference_duration = total_segment_duration
            else:
                reference_duration = None
            
            speech_ratio = (
                total_confident_duration / reference_duration
                if reference_duration and reference_duration > 0
                else None
            )
            
            avg_no_speech_prob = (
                avg_no_speech_sum / total_segment_duration
                if total_segment_duration > 0
                else 1.0
            )
            
            if reference_duration and reference_duration >= 6:
                if (
                    len(normalized_text) < self.LENGTH_LIMITS["min_audio_text"]
                    and (speech_ratio is None or speech_ratio < 0.15)
                    and total_confident_duration < 0.6
                ):
                    print(
                        "âŒ æ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³è¿‡å°‘:",
                        {
                            "expected_duration": expected_duration,
                            "total_confident_duration": total_confident_duration,
                            "speech_ratio": speech_ratio,
                            "avg_no_speech_prob": avg_no_speech_prob,
                            "segments_count": len(segments),
                        },
                    )
                    raise ValueError("æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè¯·è¯´æ¸…æ¥šä¸€äº›")
            
            # å¯¹é•¿å½•éŸ³ä¸å†ä½¿ç”¨å­—ç¬¦å¯†åº¦ç¡¬é˜ˆå€¼ï¼Œé¿å…è¯¯æ€çœŸå®å†…å®¹

            if reference_duration:
                if has_cjk:
                    # ä¸­æ–‡åœºæ™¯ï¼šç”¨æ±‰å­—æ•°é‡åˆ¤æ–­ï¼Œé¿å…â€œä¸€ä¸ªé•¿è¯â€è¢«è¯¯åˆ¤
                    if (
                        len(cjk_chars) < 3
                        and len(normalized_text) < self.LENGTH_LIMITS["min_audio_text"]
                    ):
                        print(
                            "âŒ ä¸­æ–‡æœ‰æ•ˆå­—ç¬¦è¿‡å°‘ï¼Œåˆ¤å®šä¸ºæ— æ„ä¹‰å†…å®¹:",
                            {
                                "cjk_chars": len(cjk_chars),
                                "duration": reference_duration,
                            },
                        )
                        raise ValueError("æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè¯·ç¨ä½œè¡¨è¾¾åå†è¯•")
                else:
                    if (
                        len(meaningful_tokens) < 2
                        and len(normalized_text) < self.LENGTH_LIMITS["min_audio_text"] * 2
                    ):
                        print(
                            "âŒ æœ‰æ•ˆè¯æ±‡æ•°é‡ä¸è¶³ï¼Œåˆ¤å®šä¸ºæ— æ„ä¹‰å†…å®¹:",
                            {
                                "tokens": tokens,
                                "meaningful_tokens": meaningful_tokens,
                                "duration": reference_duration,
                            },
                        )
                        raise ValueError("æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹ï¼Œè¯·ç¨ä½œè¡¨è¾¾åå†è¯•")
            
            print(f"âœ… è¯­éŸ³è¯†åˆ«æˆåŠŸ: '{text[:50]}...'")
            return text
            
        except Exception as e:
            print(f"âŒ è¯­éŸ³è½¬æ–‡å­—å¤±è´¥: {str(e)}")
            if "Invalid file format" in str(e):
                raise ValueError("éŸ³é¢‘æ ¼å¼ä¸æ”¯æŒï¼Œè¯·ä½¿ç”¨ m4a æ ¼å¼")
            elif "File too large" in str(e):
                raise ValueError("éŸ³é¢‘æ–‡ä»¶å¤ªå¤§ï¼Œè¯·æ§åˆ¶åœ¨ 2 åˆ†é’Ÿå†…")
            else:
                raise ValueError(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file_path and os.path.exists(temp_file_path):
                try:
                    os.unlink(temp_file_path)
                    print(f"ğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
                except Exception as e:
                    print(f"âš ï¸ æ¸…ç†å¤±è´¥ï¼ˆä¸å½±å“åŠŸèƒ½ï¼‰: {e}")
    
    # ========================================================================
    # ğŸ”¥ æ ¸å¿ƒæ”¹åŠ¨ï¼šæ··åˆæ¨¡å‹å¤„ç†
    # ========================================================================
    
    async def polish_content_multilingual(
        self, 
        text: str,
        user_name: Optional[str] = None,  # ç”¨æˆ·åå­—ï¼Œç”¨äºä¸ªæ€§åŒ–åé¦ˆ
        image_urls: Optional[List[str]] = None  # å›¾ç‰‡URLåˆ—è¡¨ï¼Œç”¨äºvisionåˆ†æ
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ é‡å¤§æ”¹åŠ¨ï¼šä»å•ä¸€æ¨¡å‹æ”¹ä¸ºæ··åˆæ¨¡å‹ + å¹¶è¡Œæ‰§è¡Œ
        
        æ—§é€»è¾‘ï¼š
        1. GPT-4o-mini ä¸€æ¬¡æ€§ç”Ÿæˆæ¶¦è‰² + æ ‡é¢˜ + åé¦ˆï¼ˆä¸²è¡Œï¼Œ3-5ç§’ï¼‰
        
        æ–°é€»è¾‘ï¼š
        1. GPT-4o-mini ç”Ÿæˆæ¶¦è‰² + æ ‡é¢˜ï¼ˆå­—æ®µ haikuï¼Œ1-2ç§’ï¼‰
        2. GPT-4o-mini ç”Ÿæˆåé¦ˆï¼ˆå­—æ®µ sonnetï¼ŒåŸºäºåŸå§‹æ–‡æœ¬ï¼Œ2-3ç§’ï¼‰
        3. ä¸¤ä¸ªä»»åŠ¡å¹¶è¡Œæ‰§è¡Œï¼Œæ€»è€—æ—¶ = max(1-2, 2-3) = 2-3ç§’
        
        ä¸ºä»€ä¹ˆåŸºäºåŸå§‹æ–‡æœ¬ç”Ÿæˆåé¦ˆï¼Ÿ
        - æ›´çœŸå®ï¼šåŸå§‹æ–‡æœ¬ä¿ç•™äº†ç”¨æˆ·æœ€çœŸå®çš„æƒ…æ„Ÿ
        - æ›´å¿«ï¼šä¸éœ€è¦ç­‰æ¶¦è‰²å®Œæˆ
        - æ›´æ¸©æš–ï¼šAI å›åº”"çœŸå®çš„ä½ "è€Œä¸æ˜¯"å®Œç¾çš„æ–‡å­—"
        """
        try:
            # è¾“å…¥æ£€æŸ¥
            if not text or len(text.strip()) < 5:
                raise ValueError("å†…å®¹å¤ªçŸ­ï¼Œè¯·å¤šå†™ä¸€äº›")
            
            print(f"âœ¨ å¼€å§‹AIå¤„ç†ï¼ˆå¹¶è¡Œæ¨¡å¼ï¼‰: {text[:50]}...")
            
            # ğŸ”¥ ä¼˜åŒ–è¯­è¨€æ£€æµ‹ï¼šæ›´å‡†ç¡®åœ°è¯†åˆ«ç”¨æˆ·è¾“å…¥çš„ä¸»è¦è¯­è¨€
            import re
            # ç§»é™¤ç©ºç™½å­—ç¬¦å’Œæ ‡ç‚¹ï¼Œåªç»Ÿè®¡å®é™…å†…å®¹å­—ç¬¦
            content_only = re.sub(r'[\s\W]', '', text)
            chinese_chars = 0
            english_words = 0
            
            if not content_only:
                # å¦‚æœåªæœ‰ç©ºç™½å’Œæ ‡ç‚¹ï¼Œé»˜è®¤ä½¿ç”¨ä¸­æ–‡
                detected_lang = "Chinese"
            else:
                # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
                chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content_only))
                # ç»Ÿè®¡è‹±æ–‡å­—ç¬¦ï¼ˆå•è¯ï¼‰
                english_words = len(re.findall(r'[a-zA-Z]+', content_only))
                
                # ğŸ”¥ æ–°å¢ï¼šæ£€æµ‹éŸ©è¯­/æ—¥è¯­å­—ç¬¦
                korean_chars = len(re.findall(r'[\uac00-\ud7af]', content_only))
                japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', content_only))
                
                # ğŸ”¥ è¯­è¨€ç™½åå•æ£€æŸ¥ï¼šå¦‚æœæ£€æµ‹åˆ°å¤§é‡éä¸­è‹±æ–‡å­—ç¬¦ï¼Œé™çº§åˆ°ç³»ç»Ÿé»˜è®¤è¯­è¨€
                if korean_chars > 5 or japanese_chars > 5:
                    print(f"âš ï¸ æ£€æµ‹åˆ°éæ”¯æŒè¯­è¨€å­—ç¬¦: éŸ©è¯­={korean_chars}, æ—¥è¯­={japanese_chars}")
                    print(f"   å†…å®¹: '{text[:50]}'")
                    print(f"   é™çº§åˆ°ç³»ç»Ÿé»˜è®¤è¯­è¨€: Chinese")
                    detected_lang = "Chinese"  # é™çº§åˆ°ä¸­æ–‡
                else:
                    # è®¡ç®—ä¸­æ–‡å­—ç¬¦å æ¯”
                    chinese_ratio = chinese_chars / len(content_only) if len(content_only) > 0 else 0
                    # è®¡ç®—è‹±æ–‡å•è¯å æ¯”ï¼ˆæ¯ä¸ªå•è¯å¹³å‡5ä¸ªå­—ç¬¦ä¼°ç®—ï¼‰
                    english_ratio = (english_words * 5) / len(content_only) if len(content_only) > 0 else 0
                    
                    # ğŸ”¥ å…³é”®é€»è¾‘ï¼šå¦‚æœä¸­æ–‡å­—ç¬¦å æ¯”è¶…è¿‡30%ï¼Œæˆ–è€…ä¸­æ–‡å­—ç¬¦æ•°é‡æ˜æ˜¾å¤šäºè‹±æ–‡å•è¯ï¼Œåˆ¤å®šä¸ºä¸­æ–‡
                    # è¿™æ ·å¯ä»¥é¿å…"å°‘é‡ä¸­æ–‡+å¤§é‡è‹±æ–‡"è¢«è¯¯åˆ¤ä¸ºè‹±æ–‡çš„æƒ…å†µ
                    if chinese_ratio > 0.3 or (chinese_chars > 5 and chinese_chars > english_words * 2):
                        detected_lang = "Chinese"
                    elif english_ratio > 0.5 or english_words > 10:
                        detected_lang = "English"
                    else:
                        # é»˜è®¤ï¼šå¦‚æœä¸­æ–‡å­—ç¬¦å­˜åœ¨ä¸”æ•°é‡>=3ï¼Œåˆ¤å®šä¸ºä¸­æ–‡
                        detected_lang = "Chinese" if chinese_chars >= 3 else "English"
            
            print(f"ğŸŒ æ£€æµ‹åˆ°è¯­è¨€: {detected_lang} (ä¸­æ–‡å­—ç¬¦={chinese_chars}, è‹±æ–‡å•è¯={english_words})")
            
            # ğŸ”¥ å…³é”®æ”¹åŠ¨ï¼šå¹¶è¡Œæ‰§è¡Œä¸¤ä¸ªä»»åŠ¡
            print(f"ğŸš€ å¯åŠ¨å¹¶è¡Œå¤„ç†...")
            if image_urls and len(image_urls) > 0:
                print(f"   - æ£€æµ‹åˆ° {len(image_urls)} å¼ å›¾ç‰‡ï¼Œå°†ä½¿ç”¨ Vision èƒ½åŠ›åˆ†æå›¾ç‰‡+æ–‡å­—")
            print(f"   - ä»»åŠ¡1: GPT-4o-mini æ¶¦è‰² + æ ‡é¢˜ï¼ˆå­—æ®µ haikuï¼‰")
            print(f"   - ä»»åŠ¡2: GPT-4o-mini æš–å¿ƒåé¦ˆï¼ˆå­—æ®µ sonnetï¼ŒåŸºäºåŸå§‹æ–‡æœ¬ï¼‰")
            
            # ğŸ”¥ æ€§èƒ½ä¼˜åŒ–ï¼šé¢„å…ˆä¸‹è½½å¹¶ç¼–ç æ‰€æœ‰å›¾ç‰‡ï¼Œé¿å…åœ¨å¹¶è¡Œä»»åŠ¡ä¸­é‡å¤ä¸‹è½½
            encoded_images = []
            if image_urls and len(image_urls) > 0:
                print(f"ğŸ–¼ï¸ é¢„å¤„ç† {len(image_urls)} å¼ å›¾ç‰‡...")
                # å¹¶è¡Œä¸‹è½½å›¾ç‰‡
                download_tasks = [self._download_and_encode_image(url) for url in image_urls]
                results = await asyncio.gather(*download_tasks, return_exceptions=True)
                for i, img_data in enumerate(results):
                    if isinstance(img_data, Exception):
                        print(f"âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥ ({image_urls[i]}): {img_data}")
                    else:
                        encoded_images.append(img_data)
            
            # åˆ›å»ºä¸¤ä¸ªå¼‚æ­¥ä»»åŠ¡
            polish_task = self._call_gpt4o_mini_for_polish_and_title(text, detected_lang, encoded_images)
            feedback_task = self._call_gpt4o_mini_for_feedback(text, detected_lang, user_name, encoded_images)
            
            # å¹¶è¡Œæ‰§è¡Œå¹¶ç­‰å¾…ç»“æœ
            polish_result, feedback_data = await asyncio.gather(
                polish_task,
                feedback_task
            )
            
            print(f"âœ… å¹¶è¡Œå¤„ç†å®Œæˆ")
            
            # å¤„ç†åé¦ˆç»“æœ (å…¼å®¹æ—§é€»è¾‘)
            if isinstance(feedback_data, dict):
                feedback_text = feedback_data.get("reply", "")
                emotion_data = feedback_data
            else:
                feedback_text = str(feedback_data)
                emotion_data = {"emotion": "Reflective", "confidence": 0.0}
            
            # åˆå¹¶ç»“æœ
            result = {
                "title": polish_result['title'],
                "polished_content": polish_result['polished_content'],
                "feedback": feedback_text,
                "emotion_data": emotion_data # âœ… æ–°å¢æƒ…ç»ªæ•°æ®
            }
            
            # è´¨é‡æ£€æŸ¥
            result = self._validate_and_fix_result(result, text)
            
            print(f"âœ… å¤„ç†å®Œæˆ:")
            print(f"  - æ ‡é¢˜: {result['title']}")
            print(f"  - å†…å®¹é•¿åº¦: {len(result['polished_content'])} å­—")
            print(f"  - åé¦ˆé•¿åº¦: {len(result['feedback'])} å­—")
            print(f"  - æƒ…ç»ª: {result.get('emotion_data', {}).get('emotion', 'Unknown')}")
            
            return result
        
        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)
            print(f"âŒ AIå¤„ç†å¤±è´¥: {error_type}: {error_msg}")
            import traceback
            error_trace = traceback.format_exc()
            print(f"ğŸ“ å®Œæ•´é”™è¯¯å †æ ˆ:")
            print(error_trace)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¹¶è¡Œä»»åŠ¡ä¸­çš„é”™è¯¯
            if isinstance(e, (asyncio.TimeoutError, asyncio.CancelledError)):
                print(f"âš ï¸ å¹¶è¡Œä»»åŠ¡è¶…æ—¶æˆ–å–æ¶ˆ")
            elif isinstance(e, Exception):
                print(f"âš ï¸ å¹¶è¡Œä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            
            return self._create_fallback_result(text)
    
    # ========================================================================
    # ğŸ”¥ GPT-4o-mini è°ƒç”¨ï¼ˆæ¶¦è‰² + æ ‡é¢˜ï¼‰
    # ========================================================================
    
    async def _call_gpt4o_mini_for_polish_and_title(
        self, 
        text: str,
        language: str,
        encoded_images: Optional[List[str]] = None
    ) -> Dict[str, str]:
        """
        è°ƒç”¨ GPT-4o-mini è¿›è¡Œæ¶¦è‰²å’Œç”Ÿæˆæ ‡é¢˜
        
        ğŸ“š å­¦ä¹ ç‚¹ï¼šè¿™ä¸ªå‡½æ•°è´Ÿè´£ä¸¤ä¸ªä»»åŠ¡
        1. æ¶¦è‰²ç”¨æˆ·çš„åŸå§‹æ–‡æœ¬ï¼ˆä¿®å¤è¯­æ³•ã€ä¼˜åŒ–è¡¨è¾¾ï¼‰
        2. ç”Ÿæˆä¸€ä¸ªç®€æ´æœ‰æ„ä¹‰çš„æ ‡é¢˜
        
        ä¸ºä»€ä¹ˆä½¿ç”¨ GPT-4o-miniï¼Ÿ
        - é€Ÿåº¦å¿«ï¼ˆ1-2ç§’ï¼‰
        - æˆæœ¬ä½ï¼ˆ$1/1M tokens inputï¼‰
        - è´¨é‡è¶³å¤Ÿï¼ˆæ—¥è®°æ¶¦è‰²ç»°ç»°æœ‰ä½™ï¼‰
        
        è¿”å›:
            {
                "title": "æ ‡é¢˜",
                "polished_content": "æ¶¦è‰²åçš„å†…å®¹"
            }
        """
        try:
            print(f"ğŸ¨ GPT-4o-mini: å¼€å§‹æ¶¦è‰²å’Œç”Ÿæˆæ ‡é¢˜...")
            
            # ğŸ”¥ ä¼˜åŒ–ï¼šæ ¹æ®ä¼ å…¥çš„ language å‚æ•°æ„å»ºæ›´ä¸¥æ ¼çš„ prompt
            # æ ¸å¿ƒåŸåˆ™ï¼šæ ‡é¢˜è¯­è¨€å¿…é¡»ä¸ç”¨æˆ·è¾“å…¥å†…å®¹çš„ä¸»è¦è¯­è¨€å®Œå…¨ä¸€è‡´
            language_instruction = ""
            if language == "Chinese":
                language_instruction = """ğŸš¨ CRITICAL LANGUAGE RULE - YOU MUST FOLLOW:
The user's content is primarily in CHINESE (ç®€ä½“ä¸­æ–‡). 

MANDATORY REQUIREMENTS:
1. **Title MUST be in Chinese (ç®€ä½“ä¸­æ–‡) ONLY** - NO English, NO Japanese, NO Korean
2. **Title language must match the user's input language** - If user writes in Chinese, title MUST be Chinese
3. Even if the content contains some English words or other languages, the title MUST be in Chinese
4. Polished content should preserve the original language of each part, but the title MUST be Chinese

WRONG Examples (DO NOT DO THIS):
- User input in Chinese â†’ Title: "Reflections on..." âŒ
- User input in Chinese â†’ Title: "ã‚ªãƒ¬ãƒ³ã‚¸ã®é­…åŠ›" âŒ

CORRECT Examples:
- User input: "æˆ‘å…ˆè¯•ä¸€ä¸‹è¯­éŸ³è¾“å…¥ï¼Œç°åœ¨æ€ä¹ˆæ ·" â†’ Title: "è¯­éŸ³è¾“å…¥çš„å°è¯•" âœ…
- User input: "ã‚ªãƒ¬ãƒ³ã‚¸ã®é­…åŠ› Talking about orange..." â†’ Title: "æ©™å­çš„é­…åŠ›" âœ… (Chinese, not Japanese)"""
            elif language == "English":
                language_instruction = """ğŸš¨ CRITICAL LANGUAGE RULE - YOU MUST FOLLOW:
The user's content is primarily in ENGLISH.

MANDATORY REQUIREMENTS:
1. **Title MUST be in English ONLY** - NO Chinese, NO Japanese, NO Korean
2. **Title language must match the user's input language** - If user writes in English, title MUST be English
3. Even if the content contains some Chinese words or other languages, the title MUST be in English
4. Polished content should preserve the original language of each part, but the title MUST be English

WRONG Examples (DO NOT DO THIS):
- User input in English â†’ Title: "ä»Šæ—¥è®°å½•" âŒ
- User input in English â†’ Title: "ã‚ªãƒ¬ãƒ³ã‚¸ã®é­…åŠ›" âŒ

CORRECT Examples:
- User input: "today was good i went to park" â†’ Title: "A Day at the Park" âœ…
- User input: "ã‚ªãƒ¬ãƒ³ã‚¸ã®é­…åŠ› Talking about orange..." â†’ Title: "The Charm of Oranges" âœ… (English, not Japanese)

ğŸ¯ SPECIAL POLISHING RULES FOR ENGLISH (Non-Native Speaker Support):
**PRIORITY ORDER (CRITICAL - Follow this exact sequence):**
1. **PRIMARY GOAL**: Transform the text to sound like a native English speaker wrote it
   - Eliminate ALL non-native patterns, awkward phrasing, and "foreign feel"
   - Use natural idioms, collocations, and sentence structures that native speakers use
   - Make it flow smoothly and authentically in English
   
2. **SECONDARY GOAL**: While maintaining native fluency, preserve the user's intended meaning
   - Keep the core message, emotions, and key details intact
   - Don't add information the user didn't express
   - If there's a conflict between sounding native and preserving exact wording, ALWAYS choose native fluency

3. **EDUCATIONAL VALUE**: Your polished version should serve as a learning example
   - Non-native speakers will compare your version to their original to improve their English
   - Use this as an opportunity to demonstrate natural, idiomatic English

ğŸ“‹ COMMON NON-NATIVE PATTERNS TO FIX:
- Missing articles (a/an/the): "I went to park" â†’ "I went to the park"
- Wrong prepositions: "in the morning of Monday" â†’ "on Monday morning"
- Unnatural word order: "I very like it" â†’ "I really like it" or "I like it a lot"
- Literal translations: "eat medicine" â†’ "take medicine"
- Overly formal/textbook language: "I am feeling very happy" â†’ "I'm so happy" or "I feel great"
- Choppy sentences: "I went to store. I bought milk. I came home." â†’ "I went to the store, bought some milk, and came home."
- Missing contractions in casual writing: "I am going to" â†’ "I'm going to" or "I'm gonna"
- Awkward tense usage: "Today I am going to park" (when it already happened) â†’ "I went to the park today"

âœ¨ NATIVE ENGLISH ENHANCEMENT TECHNIQUES:
- Use contractions naturally (I'm, don't, can't, it's) in casual diary entries
- Apply common phrasal verbs: "continue" â†’ "keep going", "understand" â†’ "figure out"
- Add natural filler words when appropriate: "well", "so", "anyway", "actually"
- Use idiomatic expressions: "very tired" â†’ "exhausted" or "beat", "very happy" â†’ "thrilled" or "over the moon"
- Vary sentence structure for better flow (mix short and long sentences)
- Use more specific, vivid vocabulary: "good" â†’ "great/wonderful/fantastic", "bad" â†’ "rough/tough/awful"

ğŸ” BEFORE/AFTER EXAMPLES:

Example 1 - Basic Grammar + Natural Flow:
âŒ Original: "today i go to park and see many flower it make me very happy"
âœ… Polished: "I went to the park today and saw so many flowersâ€”it made me really happy!"
(Fixed: capitalization, tense, articles, added natural emphasis with "so many", used contraction-like flow)

Example 2 - Removing Non-Native Patterns:
âŒ Original: "I am very like this new job because can learn many things"
âœ… Polished: "I really love this new job because I'm learning so much!"
(Fixed: "very like"â†’"really love", added subject "I'm", "many things"â†’"so much", more natural enthusiasm)

Example 3 - Idiomatic Enhancement:
âŒ Original: "Today weather is not good so I stay at house and do nothing"
âœ… Polished: "The weather was terrible today, so I just stayed home and did nothing."
(Fixed: added article "the", "not good"â†’"terrible", "at house"â†’"home", added natural "just")

Example 4 - Voice Input (Casual Speech):
âŒ Original: "um i think i want to try this voice input thing lets see how it work"
âœ… Polished: "Um, I think I want to try this voice input thing. Let's see how it works!"
(Fixed: punctuation, capitalization, "work"â†’"works", kept casual "um" as it's authentic)

Example 5 - Preserving Meaning While Improving Flow:
âŒ Original: "I have one meeting today. The meeting is very boring. I don't like the meeting. After meeting I feel tired."
âœ… Polished: "I had a meeting today, and it was so boring. I really didn't like it, and afterwards I felt exhausted."
(Combined choppy sentences, varied structure, used more natural vocabulary, maintained all original meaning)

âš ï¸ WHAT NOT TO CHANGE:
- Don't change the emotional tone (if user is casual, keep it casual; if formal, keep formal)
- Don't add details or experiences the user didn't mention
- Don't remove important information to make it "sound better"
- Don't over-polish to the point it doesn't sound like a diary entry anymore
- Keep proper nouns, names, and specific terms as-is (unless there's a clear typo)"""
            else:
                # é»˜è®¤ï¼šæ£€æµ‹è¯­è¨€ï¼Œä½†å¿…é¡»ä¸¥æ ¼åŒ¹é…
                language_instruction = """ğŸš¨ CRITICAL LANGUAGE RULE - YOU MUST FOLLOW:
Detect the user's PRIMARY language from their input content.

MANDATORY REQUIREMENTS:
1. **Title language MUST match the user's primary input language**
2. If content is primarily Chinese â†’ Title MUST be Chinese
3. If content is primarily English â†’ Title MUST be English
4. If content contains mixed languages, use the language that appears MOST FREQUENTLY
5. NEVER use Japanese or Korean for titles unless the ENTIRE content is in that language
6. **DO NOT mix languages in the title** - Use ONE language only, matching the user's primary language

Examples:
- User input: "ä»Šå¤©å¤©æ°”å¾ˆå¥½" (Chinese) â†’ Title: "ç¾å¥½çš„å¤©æ°”" âœ… (Chinese)
- User input: "today was good" (English) â†’ Title: "A Good Day" âœ… (English)
- User input: "ä»Šå¤©å¤©æ°”å¾ˆå¥½ today was good" (mixed, more Chinese) â†’ Title: "ç¾å¥½çš„ä¸€å¤©" âœ… (Chinese, matching primary language)"""
            
            # æ„å»º prompt
            system_prompt = f"""You are a gentle diary editor. Your task is to polish the user's diary entry and create a title.

{language_instruction}

Your responsibilities:
1. **For ENGLISH input (non-native speakers):**
   - PRIMARY: Make it sound like a native English speaker wrote it (eliminate all non-native patterns)
   - SECONDARY: Preserve the user's intended meaning and emotions
   - GOAL: Help users learn natural English by providing an exemplary polished version
   
2. **For OTHER languages (Chinese, etc.):**
   - Fix obvious grammar/typos
   - Make the text flow naturally
   - Keep it authentic and close to the original style

3. **Universal rules:**
   - Keep polished content â‰¤115% of original length
   - **CRITICAL: Preserve ALL original content. Do NOT delete or omit any part of the user's entry.**
   - **Formatting: Preserve the user's line breaks, blank lines, and bullet/numbered lists. Do NOT merge everything into one paragraph.**
   - **If the input is long and mostly one block (no line breaks), add clear paragraph breaks based on meaning.**
   - **Avoid overly short paragraphs. Do NOT break right after the first sentence. Keep the first 3 sentences in the same paragraph when you add breaks.**
   
4. **ğŸš¨ MOST CRITICAL: Create a title in the EXACT SAME LANGUAGE as the user's primary input language**
   - If user writes in Chinese â†’ Title MUST be in Chinese
   - If user writes in English â†’ Title MUST be in English
   - The title language must match the content language - NO EXCEPTIONS
   - Title should be short, warm, poetic, and meaningful, but ALWAYS in the user's language
   
5. **ğŸš¨ TITLE CONTENT RULES - AVOID GENERIC AND REDUNDANT TITLES:**
   - **NEVER use "ä»Šæ—¥" (today) in Chinese titles** - It's too generic and meaningless
   - **NEVER use "Today's..." in English titles** - Same reason, too generic
   - **If you must reference the day, use specific date format instead**: "1æœˆ9æ—¥" (Jan 9), not "ä»Šæ—¥"
   - **AVOID repeating the first line of content in the title** - The title should complement, not duplicate
   - **Be specific and meaningful**: Extract the core theme, emotion, or key event from the content
   
   **ğŸ¯ SPECIAL RULE FOR TASK LISTS AND PLANNING CONTENT:**
   - **For task lists, to-do lists, or planning content (ä»»åŠ¡æ¸…å•, è®¡åˆ’, to-do, plan, goal):**
     - **MUST include the specific date in the title** to make it informative and unique
     - Use format: "1æœˆ9æ—¥ + theme" (Chinese) or "Jan 9 + theme" (English)
     - This prevents repetitive titles like "ä»»åŠ¡æ¸…å•" appearing multiple times
   
   Examples of BAD titles (DO NOT USE):
   âŒ "ä»Šæ—¥ä»»åŠ¡æ¸…å•" - Generic "today" + redundant with content's first line "ä»Šæ—¥ä»»åŠ¡:"
   âŒ "ä»»åŠ¡æ¸…å•" - Too generic, will repeat for every task list entry
   âŒ "ä»Šæ—¥è®°å½•" - Too generic, no meaning
   âŒ "Today's Thoughts" - Generic "today"
   âŒ "Task List" - Too generic, will repeat
   
   Examples of GOOD titles:
   âœ… "1æœˆ9æ—¥ä»»åŠ¡æ¸…å•" - Specific date + clear theme, won't repeat
   âœ… "Jan 9 Task List" - Specific date + clear theme
   âœ… "1æœˆ9æ—¥çš„Appä¸Šæ¶è®¡åˆ’" - Date + specific goal
   âœ… "App Storeä¸Šæ¶è®¡åˆ’" - Specific, captures the main theme (if not a generic task list)
   âœ… "è¿ˆå‘æ–°ç›®æ ‡" - Meaningful, captures the essence

Style Guidelines:
- **For English**: Natural, fluent, native-sounding. Prioritize authenticity over preserving awkward phrasing.
- **For Chinese**: Natural, warm, authentic. Don't over-edit.
- **For all**: Keep the emotional tone and diary-like feel.

Response format (JSON only):
{{
  "title": "Title in the EXACT SAME LANGUAGE as the user's primary input (Chinese or English only - MUST match user's language)",
  "polished_content": "fixed text, preserving original language AND original formatting (line breaks/lists) - MUST include all original content"
}}

ğŸš¨ CRITICAL EXAMPLES - Study these carefully:

Example 1 (User writes in Chinese - Title MUST be Chinese):
Input: "æˆ‘å…ˆè¯•ä¸€ä¸‹è¯­éŸ³è¾“å…¥ï¼Œç°åœ¨æ€ä¹ˆæ ·ã€‚å“å‘€ï¼Œå°±æ˜¯æœ‰ç‚¹å¤±è½ï¼Œå› ä¸ºæ˜æ˜åº”è¯¥æ—©ç‚¹ç¡çš„ã€‚"
Output: {{"title": "å¤±çœ çš„å¤œæ™š", "polished_content": "æˆ‘å…ˆè¯•ä¸€ä¸‹è¯­éŸ³è¾“å…¥ï¼Œç°åœ¨æ€ä¹ˆæ ·ã€‚å“å‘€ï¼Œå°±æ˜¯æœ‰ç‚¹å¤±è½ï¼Œå› ä¸ºæ˜æ˜åº”è¯¥æ—©ç‚¹ç¡çš„ã€‚"}}
âŒ WRONG: {{"title": "Reflections on Sleepless Nights"}} - This is English, but user wrote in Chinese!

Example 2 (User writes in English - Title MUST be English):
Input: "today was good i went to park and saw many flowers"
Output: {{"title": "A Day at the Park", "polished_content": "Today was good. I went to the park and saw many flowers."}}
âŒ WRONG: {{"title": "å…¬å›­ä¸€æ—¥"}} - This is Chinese, but user wrote in English!

Example 3 (User writes in Chinese with some English words - Title MUST be Chinese):
Input: "ä»Šå¤©å»äº†parkï¼Œçœ‹åˆ°äº†å¾ˆå¤šflowersï¼Œå¿ƒæƒ…å¾ˆå¥½"
Output: {{"title": "å…¬å›­é‡Œçš„èŠ±", "polished_content": "ä»Šå¤©å»äº†parkï¼Œçœ‹åˆ°äº†å¾ˆå¤šflowersï¼Œå¿ƒæƒ…å¾ˆå¥½ã€‚"}}
âœ… CORRECT: Title is in Chinese because user's primary language is Chinese

Example 4 (User writes in English with some Chinese words - Title MUST be English):
Input: "I went to å…¬å›­ today and saw many èŠ±"
Output: {{"title": "A Visit to the Park", "polished_content": "I went to å…¬å›­ today and saw many èŠ±."}}
âœ… CORRECT: Title is in English because user's primary language is English"""

            # æ„å»ºç”¨æˆ·æ¶ˆæ¯å†…å®¹
            user_content = []
            
            # å¦‚æœæœ‰å›¾ç‰‡ï¼Œæ·»åŠ å›¾ç‰‡åˆ°æ¶ˆæ¯ä¸­ï¼ˆä½¿ç”¨visionèƒ½åŠ›ï¼‰
            if encoded_images and len(encoded_images) > 0:
                print(f"ğŸ–¼ï¸ æ·»åŠ  {len(encoded_images)} å¼ å›¾ç‰‡åˆ° Vision è¯·æ±‚ (Low-res æ¨¡å¼)...")
                for image_data in encoded_images:
                    user_content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_data}",
                            "detail": "low"  # âœ… ä½¿ç”¨ä½åˆ†è¾¨ç‡æ¨¡å¼ï¼Œå¤„ç†æ›´å¿«ä¸”æ›´çœé’±
                        }
                    })
                
                # æ·»åŠ æ–‡å­—å†…å®¹
                user_content.append({
                    "type": "text",
                    "text": f"Please polish this diary entry (preserve ALL content) and create a title. Consider both the images and the text:\n\n{text}"
                })
                user_prompt = user_content
            else:
                # åªæœ‰æ–‡å­—ï¼Œä½¿ç”¨çº¯æ–‡æœ¬
                user_prompt = f"Please polish this diary entry (preserve ALL content):\n\n{text}"
            
            # âœ… åŠ¨æ€è®¡ç®— max_tokensï¼šç¡®ä¿è¶³å¤Ÿè¾“å‡ºå®Œæ•´å†…å®¹
            # åŸå§‹æ–‡æœ¬é•¿åº¦ + æ ‡é¢˜ + JSON æ ¼å¼å¼€é”€ + å®‰å…¨è¾¹è·
            original_length = len(text)
            # å¦‚æœæœ‰å›¾ç‰‡ï¼Œéœ€è¦é¢å¤–çš„tokensï¼ˆæ¯å¼ å›¾ç‰‡çº¦85 tokensï¼‰
            image_tokens = len(encoded_images) * 85 if encoded_images else 0
            # ä¼°ç®—ï¼šåŸå§‹æ–‡æœ¬ * 1.15ï¼ˆ115%é™åˆ¶ï¼‰ + æ ‡é¢˜ï¼ˆ50å­—ç¬¦ï¼‰ + JSONæ ¼å¼ï¼ˆ100å­—ç¬¦ï¼‰ + å®‰å…¨è¾¹è·ï¼ˆ500å­—ç¬¦ï¼‰
            estimated_output_length = int(original_length * 1.15) + 50 + 100 + 500
            # max_tokens å¤§çº¦æ˜¯å­—ç¬¦æ•°çš„ 0.75ï¼ˆä¸­æ–‡ï¼‰åˆ° 1.5ï¼ˆè‹±æ–‡ï¼‰ï¼Œå–ä¸­é—´å€¼ 1.0
            max_tokens = max(2000, int(estimated_output_length * 1.0) + image_tokens)
            # ä½†ä¸è¦è¶…è¿‡ OpenAI çš„é™åˆ¶ï¼ˆGPT-4o-mini æ”¯æŒ 16384 tokensï¼‰
            max_tokens = min(max_tokens, 16000)
            
            print(f"ğŸ“¤ GPT-4o-mini: å‘é€è¯·æ±‚åˆ° OpenAI...")
            print(f"   æ¨¡å‹: {self.MODEL_CONFIG['haiku']}")
            print(f"   åŸå§‹æ–‡æœ¬é•¿åº¦: {original_length} å­—ç¬¦")
            print(f"   å›¾ç‰‡æ•°é‡: {len(encoded_images) if encoded_images else 0}")
            print(f"   ä¼°ç®—è¾“å‡ºé•¿åº¦: {estimated_output_length} å­—ç¬¦")
            print(f"   è®¾ç½® max_tokens: {max_tokens}")
            
            # æ„å»ºæ¶ˆæ¯
            if encoded_images and len(encoded_images) > 0:
                # ä½¿ç”¨visionæ ¼å¼ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            else:
                # çº¯æ–‡æœ¬æ ¼å¼
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            
            # ä½¿ç”¨ OpenAI clientï¼ˆå·²ç»åœ¨ __init__ ä¸­åˆå§‹åŒ–ï¼‰
            response = await asyncio.to_thread(
                self.openai_client.chat.completions.create,
                model=self.MODEL_CONFIG["haiku"],
                messages=messages,
                temperature=0.3,
                max_tokens=max_tokens,
                response_format={"type": "json_object"}  # å¼ºåˆ¶ JSON æ ¼å¼
            )
            
            # è§£æå“åº”
            content = response.choices[0].message.content
            if not content:
                raise ValueError("OpenAI è¿”å›ç©ºå“åº”")
            
            print(f"âœ… GPT-4o-mini: æ”¶åˆ°å“åº”")
            print(f"ğŸ“ GPT-4o-mini: å“åº”å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            
            # è§£æ JSON
            try:
                result = json.loads(content)
                polished_content = result.get("polished_content", text)
                
                # âœ… æ·»åŠ é•¿åº¦å¯¹æ¯”æ—¥å¿—ï¼Œæ£€æŸ¥æ˜¯å¦è¢«æˆªæ–­
                original_length = len(text)
                polished_length = len(polished_content)
                length_ratio = polished_length / original_length if original_length > 0 else 0
                
                print(f"âœ… GPT-4o-mini: æ¶¦è‰²å®Œæˆ")
                print(f"ğŸ“Š é•¿åº¦å¯¹æ¯”: åŸå§‹={original_length} å­—ç¬¦, æ¶¦è‰²å={polished_length} å­—ç¬¦, æ¯”ä¾‹={length_ratio:.2%}")
                
                # âš ï¸ å¦‚æœæ¶¦è‰²åå†…å®¹æ˜æ˜¾å°‘äºåŸå§‹å†…å®¹ï¼ˆå°äº80%ï¼‰ï¼Œå¯èƒ½æ˜¯è¢«æˆªæ–­äº†
                if polished_length < original_length * 0.8:
                    print(f"âš ï¸ è­¦å‘Šï¼šæ¶¦è‰²åå†…å®¹æ˜æ˜¾å°‘äºåŸå§‹å†…å®¹ï¼Œå¯èƒ½è¢«æˆªæ–­ï¼")
                    print(f"   åŸå§‹å†…å®¹å‰100å­—ç¬¦: {text[:100]}...")
                    print(f"   æ¶¦è‰²åå†…å®¹å‰100å­—ç¬¦: {polished_content[:100]}...")
                    # å¦‚æœç¡®å®è¢«æˆªæ–­ï¼Œä½¿ç”¨åŸå§‹å†…å®¹ä½œä¸ºé™çº§æ–¹æ¡ˆ
                    polished_content = text
                    print(f"   ä½¿ç”¨åŸå§‹å†…å®¹ä½œä¸ºé™çº§æ–¹æ¡ˆ")
                
                return {
                    "title": result.get("title", "Today's Reflection"),
                    "polished_content": polished_content
                }
            except json.JSONDecodeError as e:
                print(f"âš ï¸ GPT-4o-mini: JSON è§£æå¤±è´¥: {e}")
                print(f"   åŸå§‹å“åº”: {content[:200]}...")
                # å°è¯•ä»æ–‡æœ¬ä¸­æå– JSON
                import re
                json_match = re.search(r'\{[^{}]*"title"[^{}]*"polished_content"[^{}]*\}', content)
                if json_match:
                    try:
                        result = json.loads(json_match.group())
                        return {
                            "title": result.get("title", "Today's Reflection"),
                            "polished_content": result.get("polished_content", text)
                        }
                    except:
                        pass
                
                # é™çº§æ–¹æ¡ˆ
                print(f"âš ï¸ GPT-4o-mini: ä½¿ç”¨é™çº§æ–¹æ¡ˆ")
                return {
                    "title": "Today's Reflection" if language == "English" else "ä»Šæ—¥è®°å½•",
                    "polished_content": text
                }
        
        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)
            print(f"âŒ GPT-4o-mini è°ƒç”¨å¤±è´¥: {error_type}: {error_msg}")
            
            # è¯¦ç»†é”™è¯¯ä¿¡æ¯
            import traceback
            error_trace = traceback.format_exc()
            print(f"ğŸ“ GPT-4o-mini å®Œæ•´é”™è¯¯å †æ ˆ:")
            print(error_trace)
            
            # æ£€æŸ¥å¸¸è§é”™è¯¯ç±»å‹
            if "RateLimitError" in error_type or "rate_limit" in error_msg.lower():
                print(f"âš ï¸ OpenAI API é™æµ: è¯·æ±‚é¢‘ç‡è¿‡é«˜")
                print(f"ğŸ’¡ å»ºè®®: ç¨åé‡è¯•ï¼Œæˆ–æ£€æŸ¥ OpenAI è´¦æˆ·çš„é…é¢é™åˆ¶")
            elif "AuthenticationError" in error_type or "InvalidApiKey" in error_type:
                print(f"âš ï¸ OpenAI API Key é”™è¯¯: è¯·æ£€æŸ¥ OPENAI_API_KEY ç¯å¢ƒå˜é‡")
            elif "APIConnectionError" in error_type:
                print(f"âš ï¸ OpenAI API è¿æ¥é”™è¯¯: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            
            # é™çº§æ–¹æ¡ˆ
            return {
                "title": "Today's Reflection" if language == "English" else "ä»Šæ—¥è®°å½•",
                "polished_content": text
            }
    
    # ========================================================================
    # ğŸ”¥ GPT-4o-mini è°ƒç”¨ï¼ˆAI åé¦ˆï¼‰
    # ========================================================================
    
    async def _call_gpt4o_mini_for_feedback(
        self, 
        text: str,
        language: str,
        user_name: Optional[str] = None,
        encoded_images: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨ GPT-4o-mini ç”Ÿæˆæ¸©æš–çš„ AI åé¦ˆ + æƒ…ç»ªåˆ†æ
        
        è¿”å›:
            {
                "reply": "æ¸©æš–çš„åé¦ˆæ–‡å­—",
                "emotion": "Joyful",
                "confidence": 0.9,
                "rationale": "åˆ†æç†ç”±..."
            }
        """
        try:
            print(f"ğŸ’¬ GPT-4o-mini: å¼€å§‹ç”Ÿæˆåé¦ˆ + æƒ…ç»ªåˆ†æ...")
            print(f"ğŸ‘¤ ç”¨æˆ·åå­—: {user_name if user_name else 'æœªæä¾›'}")
            
            # è®¡ç®—ç”¨æˆ·è¾“å…¥é•¿åº¦ï¼Œç”¨äºåŠ¨æ€è°ƒæ•´åé¦ˆé•¿åº¦
            user_text_length = len(text.strip())
            max_feedback_length = max(user_text_length, 20 if language == "Chinese" else 15)
            
            # æ„å»ºç»Ÿä¸€çš„ç³»ç»Ÿæç¤ºè¯
            # æƒ…ç»ªåˆ—è¡¨ï¼šä¸å‰ç«¯ EmotionType ä¿æŒä¸¥æ ¼ä¸€è‡´ï¼ˆ2026-01-10 æ›´æ–° v4 - æ‰©å±•åˆ°23ä¸ªæƒ…ç»ªï¼ŒReflectiveæ‹†åˆ†ä¸ºThoughtfulå’ŒReflectiveï¼‰
            # Joyful, Grateful, Fulfilled, Proud, Surprised, Excited, Peaceful, Hopeful,
            # Reflective, Intentional, Inspired, Curious, Nostalgic, Calm,
            # Uncertain, Misunderstood, Lonely, Down, Anxious, Overwhelmed, Venting, Frustrated
            system_prompt = f"""You are a warm, empathetic listener AND an expert emotion analyst.

LANGUAGE RULES:
1. Detect and Follow: Respond in THE SAME LANGUAGE as the user's input.
2. Fallback: If input is empty/images only, respond in {language}.
3. Consistency: NEVER translate. Match the emotional tone.

âš ï¸ CRITICAL RULES FOR REPLY:
1. **NEVER ask questions**: Do not ask "How are you?" or "What's on your mind?".
2. **Warm Listener**: Acknowledge their feelings with warmth and resonance.
3. **Short and Powerful**: 1-2 sentences. Concise.
4. **Greeting**: {"Start response with '" + user_name + (", " if language == "English" else "ï¼Œ") + "'." if user_name else "Start directly."}

ğŸ“Š EMOTION ANALYSIS RULES:
Analyze the user's emotion from the text/images and choose ONE from this STRICT list (23 emotions):
[Joyful, Grateful, Fulfilled, Proud, Surprised, Excited, Peaceful, Hopeful, Thoughtful, Reflective, Intentional, Inspired, Curious, Nostalgic, Calm, Uncertain, Misunderstood, Lonely, Down, Anxious, Overwhelmed, Venting, Frustrated]

ğŸ¯ Detailed Usage Guide:

**ğŸŒŸ Positive Emotions (8) - é«˜èƒ½é‡/æ­£å‘:**

- **Joyful (å–œæ‚¦)**: Pure happiness, celebration, good things happening. User expresses excitement, delight, or joy.
  Examples: "Had so much fun today!", "Laughed until my stomach hurt", "ä»Šå¤©å¤ªå¼€å¿ƒäº†"

- **Grateful (æ„Ÿæ©)**: Thankfulness towards people, events, or things. Core of gratitude journaling.
  Examples: "So thankful for my friend's help", "Grateful for this moment", "æ„Ÿè°¢å®¶äººçš„æ”¯æŒ"

- **Fulfilled (å……å®)**: âœ¨ NEW - Sense of accomplishment, achievement, productive satisfaction. Completing goals, getting results.
  Examples: "Completed my project!", "Learned a new skill today", "å®Œæˆäº†å¤§é¡¹ç›®ï¼Œå¾ˆæœ‰æˆå°±æ„Ÿ"
  Keywords: "å®Œæˆ", "è¾¾æˆ", "å®ç°", "æˆå°±", "æ”¶è·", "accomplished", "achieved", "completed"
  
- **Proud (æ¬£æ…°)**: Feeling pleased about personal growth or others' progress. For self or others.
  Examples: "My child made progress", "Overcame a challenge", "å­©å­è¿›æ­¥äº†ï¼Œå¾ˆæ¬£æ…°"
  NOTE: Use sparingly; default to Fulfilled for routine accomplishments.

- **Surprised (æƒŠå–œ)**: âœ¨ NEW - Unexpected joy, pleasant surprise, serendipity. Unplanned good things.
  Examples: "Received an unexpected gift!", "Ran into an old friend", "æ²¡æƒ³åˆ°ä¼šæ”¶åˆ°è¿™ä»½ç¤¼ç‰©"
  Keywords: "æ„å¤–", "æƒŠå–œ", "æ²¡æƒ³åˆ°", "çªç„¶", "unexpected", "surprise", "serendipity"

- **Excited (æœŸå¾…)**: âœ¨ NEW - Anticipation, looking forward to something, energized about future.
  Examples: "Can't wait for the trip!", "Starting a new project tomorrow", "å¥½æœŸå¾…æ˜å¤©çš„æ´»åŠ¨"
  Keywords: "æœŸå¾…", "ç­‰å¾…", "å³å°†", "é©¬ä¸Š", "looking forward", "can't wait", "excited about"

- **Peaceful (å¹³é™)**: Inner calm, tranquility, relaxation. No turmoil.
  Examples: "Meditated by the lake", "Quiet evening at home", "å†…å¿ƒå¾ˆå¹³é™"

- **Hopeful (å¸Œæœ›)**: âœ¨ NEW - Optimism about the future, seeing light in darkness, believing things will improve.
  Examples: "Things will get better", "Saw a glimmer of hope", "ç›¸ä¿¡æ˜å¤©ä¼šæ›´å¥½"
  Keywords: "å¸Œæœ›", "ç›¸ä¿¡", "ä¼šå¥½", "æ›™å…‰", "hope", "believe", "will get better"

**ğŸ§˜ Neutral/Constructive Emotions (7) - ç¨³æ€/å»ºè®¾æ€§:**

- **Thoughtful (è‹¥æœ‰æ‰€æ€)**: ğŸ”¥ **DEFAULT for general thinking/recording**. Pondering, considering, thinking things through. Most common neutral state for daily journaling.
  Examples: "Thinking about today", "Just recording my thoughts", "åœ¨æƒ³ä¸è®°å½•"
  Keywords: "åœ¨æƒ³", "è®°å½•", "æ€è€ƒ", "æƒ³ç€", "thoughtful", "pondering", "considering"
  NOTE: Use Thoughtful as the default neutral emotion when user is simply thinking or recording without strong emotional state.

- **Reflective (å†…çœ)**: Deep self-reflection, insights, understanding experiences and motivations. Deeper contemplation than Thoughtful.
  Examples: "Realized something important today", "Deep reflection on my life", "æ·±åº¦åæ€è‡ªå·±çš„ç»å†"
  Keywords: "æ„Ÿæ‚Ÿ", "åæ€", "å†…çœ", "æ·±åº¦", "realized", "reflection", "insights", "deep thoughts"

- **Intentional (ç¬ƒå®š)**: ğŸ”¥ **HIGHEST PRIORITY for planning content**. Goal-setting, planning, creating to-do lists.
  **MANDATORY KEYWORDS**: "è®¡åˆ’", "æ‰“ç®—", "æƒ³è¦", "è¦åš", "ç›®æ ‡", "å‡†å¤‡", "å®‰æ’", "æ›´æ–°", "plan", "goal", "to-do", "will do", "want to", "update"
  **If ANY of these keywords appear â†’ MUST choose Intentional**
  Examples: "ä»Šå¤©æˆ‘æƒ³è¦æŠŠè¿™ä¸ªäº§å“æ›´æ–°åˆ°App Store", "äº§å“æ›´æ–°è®¡åˆ’"

- **Inspired (å¯è¿ª)**: ğŸ”¥ **HIGHEST PRIORITY for learning content**. Recording learning notes, new knowledge, insights.
  **MANDATORY KEYWORDS**: "å­¦åˆ°", "å­¦ä¹ ", "å‘ç°", "äº†è§£åˆ°", "è®¤è¯†åˆ°", "æ–°çŸ¥", "è§‚ç‚¹", "å¯å‘", "learn", "discover", "realize", "insight", "knowledge", "phrase", "concept"
  **If ANY of these keywords appear â†’ MUST choose Inspired**
  Examples: "Today, I learned a new phrase", "ä»Šå¤©å­¦åˆ°ä¸€ä¸ªæ¦‚å¿µ"

- **Curious (å¥½å¥‡)**: âœ¨ NEW - Interested in exploring, desire to learn, wondering about something.
  Examples: "Want to try something new", "Curious about this topic", "å¯¹è¿™ä¸ªå¾ˆå¥½å¥‡"
  Keywords: "å¥½å¥‡", "æƒ³çŸ¥é“", "æ¢ç´¢", "å°è¯•", "curious", "wonder", "explore", "try"

- **Nostalgic (æ€€å¿µ)**: âœ¨ NEW - Reminiscing about the past, missing old times, sentimental memories.
  Examples: "Looking at old photos", "Missing childhood", "æƒ³èµ·äº†å°æ—¶å€™"
  Keywords: "æ€€å¿µ", "æƒ³èµ·", "å›å¿†", "è¿‡å»", "ä»¥å‰", "nostalgic", "remember", "miss", "old times"

- **Calm (æ·¡ç„¶)**: âœ¨ NEW - Accepting reality, letting go, equanimity. Not fighting, just accepting.
  Examples: "Let it be", "Accepting what is", "é¡ºå…¶è‡ªç„¶å§"
  Keywords: "æ·¡ç„¶", "é¡ºå…¶è‡ªç„¶", "æ¥å—", "æ”¾ä¸‹", "let go", "accept", "let it be"

**ğŸ˜” Negative/Release Emotions (7) - ä½èƒ½é‡/å®£æ³„:**

- **Uncertain (è¿·èŒ«)**: âœ¨ NEW - Self-doubt, lack of direction, confusion, not knowing what to do.
  Examples: "Don't know what to do", "Feeling lost", "ä¸çŸ¥é“è¯¥æ€ä¹ˆåŠ", "å¯¹è‡ªå·±æ²¡ä¿¡å¿ƒ"
  Keywords: "è¿·èŒ«", "ä¸çŸ¥é“", "å›°æƒ‘", "æ²¡æ–¹å‘", "æ€€ç–‘è‡ªå·±", "uncertain", "confused", "lost", "don't know"

- **Misunderstood (å§”å±ˆ)**: âœ¨ NEW - Feeling wronged, not understood, unappreciated. Efforts not seen.
  Examples: "No one understands me", "My efforts weren't seen", "æ²¡äººç†è§£æˆ‘çš„æƒ³æ³•"
  Keywords: "å§”å±ˆ", "ä¸è¢«ç†è§£", "è¯¯è§£", "ä¸å…¬å¹³", "misunderstood", "wronged", "not appreciated"

- **Lonely (å­¤ç‹¬)**: âœ¨ NEW - Lack of meaningful social connection, feeling isolated or alone. Missing companionship.
  Examples: "Feeling lonely in a new city", "Miss having someone to talk to", "ä¸€ä¸ªäººåœ¨å¼‚åœ°ï¼Œå¾ˆå­¤ç‹¬", "æ²¡äººé™ªä¼´"
  Keywords: "å­¤ç‹¬", "å­¤å•", "ä¸€ä¸ªäºº", "æ²¡äººé™ª", "æƒ³å¿µ", "lonely", "alone", "isolated", "miss company", "no one around"

- **Down (ä½è½)**: Sadness, feeling low, unhappy. General low mood.
  Examples: "Feeling sad today", "Not in a good mood", "å¿ƒæƒ…å¾ˆä½è½"

- **Anxious (ç„¦è™‘)**: Worry about the future, tension, pressure, nervousness.
  Examples: "Worried about the exam", "Nervous about the meeting", "å¾ˆç„¦è™‘"

- **Overwhelmed (ç–²æƒ«)**: âœ¨ NEW - Exhausted, burned out, too much to handle. Can't cope.
  Examples: "So tired", "Too much work", "å®Œå…¨ç´¯å®äº†", "å‹åŠ›å¤ªå¤§äº†"
  Keywords: "ç–²æƒ«", "ç´¯", "è€—ç«­", "ä¸å ªé‡è´Ÿ", "overwhelmed", "exhausted", "burned out", "too much"

- **Venting (å®£æ³„)**: Actively releasing anger, frustration, need to vent. Healthy emotional release.
  Examples: "So annoyed!", "Need to vent", "å¤ªçƒ¦äº†ï¼Œè¦åæ§½ä¸€ä¸‹"
  Keywords: "çƒ¦", "ç”Ÿæ°”", "åæ§½", "å‘æ³„", "annoyed", "frustrated", "venting", "letting it out"

- **Frustrated (å—æŒ«)**: âœ¨ NEW - Feeling blocked, plans failed, setbacks, things not working out.
  Examples: "Nothing is going right", "Plans fell through", "åŠªåŠ›äº†å¾ˆä¹…è¿˜æ˜¯æ²¡æˆåŠŸ"
  Keywords: "å—æŒ«", "å¤±è´¥", "ä¸é¡º", "é˜»ç¢", "frustrated", "setback", "didn't work", "blocked"

ğŸš¨ CRITICAL DISTINCTION RULES:

1. **Fulfilled vs Joyful**: Fulfilled = achievement/accomplishment, Joyful = pure happiness
2. **Surprised vs Excited**: Surprised = unexpected event (past), Excited = anticipation (future)
3. **Uncertain vs Down**: Uncertain = self-doubt/confusion, Down = general sadness
4. **Misunderstood vs Venting**: Misunderstood = feeling wronged, Venting = actively releasing anger
5. **Lonely vs Down**: Lonely = lack of connection/companionship, Down = general sadness
6. **Lonely vs Misunderstood**: Lonely = no one around, Misunderstood = people around but don't understand
7. **Overwhelmed vs Down**: Overwhelmed = exhausted/too much, Down = sad/low mood
8. **Frustrated vs Venting**: Frustrated = blocked/setback, Venting = releasing emotion
9. **Proud vs Fulfilled**: Proud = pleased about growth (self/others), Fulfilled = accomplished goals
10. **Thoughtful vs Reflective**: Thoughtful = general thinking/pondering (default neutral), Reflective = deep self-reflection with insights

ğŸš¨ CRITICAL EXAMPLES - STUDY THESE CAREFULLY:

1. "ä»Šå¤©å®Œæˆäº†ä¸€ä¸ªå¤§é¡¹ç›®ï¼Œå¾ˆæœ‰æˆå°±æ„Ÿï¼"
   â†’ **Fulfilled** âœ… (achievement, accomplishment)
   â†’ NOT Joyful âŒ (not pure happiness, it's about achievement)
   
2. "æ²¡æƒ³åˆ°ä¼šæ”¶åˆ°è¿™ä»½ç¤¼ç‰©ï¼Œå¤ªæƒŠå–œäº†ï¼"
   â†’ **Surprised** âœ… (unexpected, pleasant surprise)
   â†’ NOT Joyful âŒ (emphasis on unexpectedness)
   
3. "ä¸çŸ¥é“è¯¥æ€ä¹ˆåŠï¼Œå¾ˆè¿·èŒ«"
   â†’ **Uncertain** âœ… (self-doubt, lack of direction)
   â†’ NOT Down âŒ (not general sadness, specific confusion)
   
4. "æ²¡äººç†è§£æˆ‘çš„æƒ³æ³•ï¼Œå¾ˆå§”å±ˆ"
   â†’ **Misunderstood** âœ… (feeling wronged, not understood)
   â†’ NOT Venting âŒ (not actively releasing anger)
   
5. "ä»Šå¤©æˆ‘æƒ³è¦æŠŠè¿™ä¸ªäº§å“æ›´æ–°åˆ°App Store"
   â†’ **Intentional** âœ… (planning keywords: "æƒ³è¦", "æ›´æ–°")
   â†’ NOT Fulfilled âŒ (planning future, not completed yet)

Response format (JSON ONLY):
{{
  "reply": "Your warm response text here...",
  "emotion": "Selected Emotion from list",
  "confidence": 0.9,
  "rationale": "Short reason for analysis"
}}"""


            # æ„å»ºæ¶ˆæ¯
            user_content = []
            if encoded_images and len(encoded_images) > 0:
                for image_data in encoded_images:
                    user_content.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_data}", "detail": "low"}
                    })
                user_content.append({"type": "text", "text": f"Analyze emotion and respond to this (including images):\n\n{text}"})
                messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_content}]
            else:
                messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": f"Analyze emotion and respond to this:\n\n{text}"}]

            # å¢åŠ  max_tokens ä»¥å®¹çº³ JSON
            estimated_output_length = max_feedback_length + 200 
            max_tokens = max(300, min(estimated_output_length, 1000))

            response = await asyncio.to_thread(
                self.openai_client.chat.completions.create,
                model=self.MODEL_CONFIG["sonnet"], # ç»§ç»­ä½¿ç”¨é…ç½®å¥½çš„æ¨¡å‹
                messages=messages,
                temperature=0.7,
                max_tokens=max_tokens,
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content
            if not content:
                raise ValueError("OpenAI è¿”å›ç©ºå“åº”")

            try:
                result = json.loads(content)
                reply = result.get("reply", "").strip()
                emotion = result.get("emotion", "Reflective")
                
                # âœ… æ·»åŠ è°ƒè¯•æ—¥å¿—
                print(f"ğŸ” [DEBUG] åå­—å‰ç¼€æ£€æŸ¥:")
                print(f"   user_name å‚æ•°: '{user_name}'")
                print(f"   AI åŸå§‹å›å¤: '{reply}'")
                
                # åå­—å‰ç¼€æ£€æŸ¥
                if user_name and user_name.strip():
                    trimmed_reply = reply.lstrip()
                    if not trimmed_reply.lower().startswith(user_name.lower()):
                        import re
                        has_cjk = bool(re.search(r'[\u4e00-\u9fff]', trimmed_reply))
                        separator = "ï¼Œ" if has_cjk else ", "
                        reply = f"{user_name}{separator}{trimmed_reply}"
                
                result["reply"] = reply
                print(f"âœ… åé¦ˆç”Ÿæˆ: {reply[:30]}... (Mood: {emotion})")
                return result
                
            except json.JSONDecodeError:
                print("âš ï¸ JSON è§£æå¤±è´¥ï¼Œå›é€€åˆ°çº¯æ–‡æœ¬å¤„ç†")
                return {
                    "reply": content.strip(),
                    "emotion": "Reflective", 
                    "confidence": 0.5,
                    "rationale": "Extracted from non-JSON response"
                }
        
        except Exception as e:
            print(f"âŒ åé¦ˆç”Ÿæˆå¤±è´¥: {e}")
            fallback_reply = "æ„Ÿè°¢åˆ†äº«ä½ çš„è¿™ä¸€åˆ»ã€‚" if language == "Chinese" else "Thanks for sharing this moment."
            return {
                "reply": fallback_reply,
                "emotion": "Reflective",
                "confidence": 0.0,
                "rationale": "Fallback due to error"
            }
    
    # ========================================================================
    # éªŒè¯å’Œé™çº§é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
    # ========================================================================
    
    def _validate_and_fix_result(
        self, 
        result: Dict[str, str], 
        original_text: str
    ) -> Dict[str, str]:
        """
        éªŒè¯å¹¶ä¿®æ­£AIè¾“å‡º - è´¨é‡æŠŠå…³
        
        ğŸ”¥ æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•å®Œå…¨ä¿æŒä¸å˜
        """
        import re
        
        orig_len = len(original_text.strip())
        
        # æ£€æµ‹è¯­è¨€
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', original_text))
        is_chinese = chinese_chars > len(original_text) * 0.2
        
        print(f"ğŸ“Š åŸæ–‡è¯­è¨€æ£€æµ‹: æ€»é•¿åº¦={len(original_text)}, ä¸­æ–‡å­—ç¬¦={chinese_chars}, åˆ¤å®š={'ä¸­æ–‡' if is_chinese else 'è‹±æ–‡'}")
        
        # æå–å„éƒ¨åˆ†
        title = (result.get("title", "") or "").strip()
        polished = (result.get("polished_content", "") or "").strip()
        feedback = (result.get("feedback", "") or "").strip()
        emotion_data = result.get("emotion_data", {"emotion": "Reflective"}) # âœ… ä¿ç•™æƒ…ç»ªæ•°æ®
        
        # ğŸ”¥ å¼ºåŒ–è¯­è¨€ä¸€è‡´æ€§éªŒè¯ï¼šæ›´å‡†ç¡®åœ°æ£€æµ‹å’Œä¿®æ­£
        title_has_chinese = bool(re.search(r'[\u4e00-\u9fff]', title))
        title_has_english = bool(re.search(r'[a-zA-Z]', title))
        feedback_has_chinese = bool(re.search(r'[\u4e00-\u9fff]', feedback))
        
        used_fallback = False
        
        # ğŸ”¥ æ›´ä¸¥æ ¼çš„æ ‡é¢˜è¯­è¨€æ£€æŸ¥
        # å¦‚æœç”¨æˆ·è¾“å…¥æ˜¯ä¸­æ–‡ï¼Œä½†æ ‡é¢˜åŒ…å«è‹±æ–‡ä¸”æ²¡æœ‰ä¸­æ–‡ï¼Œåˆ¤å®šä¸ºä¸ä¸€è‡´
        # å¦‚æœç”¨æˆ·è¾“å…¥æ˜¯è‹±æ–‡ï¼Œä½†æ ‡é¢˜åŒ…å«ä¸­æ–‡ä¸”æ²¡æœ‰è‹±æ–‡ï¼Œåˆ¤å®šä¸ºä¸ä¸€è‡´
        title_language_mismatch = False
        if is_chinese:
            # ç”¨æˆ·è¾“å…¥æ˜¯ä¸­æ–‡ï¼Œæ ‡é¢˜åº”è¯¥æ˜¯ä¸­æ–‡
            if not title_has_chinese and title_has_english:
                title_language_mismatch = True
                print(f"âš ï¸ æ ‡é¢˜è¯­è¨€ä¸ä¸€è‡´ï¼ç”¨æˆ·è¾“å…¥æ˜¯ä¸­æ–‡ï¼Œä½†æ ‡é¢˜æ˜¯è‹±æ–‡: '{title}'")
        else:
            # ç”¨æˆ·è¾“å…¥æ˜¯è‹±æ–‡ï¼Œæ ‡é¢˜åº”è¯¥æ˜¯è‹±æ–‡
            if not title_has_english and title_has_chinese:
                title_language_mismatch = True
                print(f"âš ï¸ æ ‡é¢˜è¯­è¨€ä¸ä¸€è‡´ï¼ç”¨æˆ·è¾“å…¥æ˜¯è‹±æ–‡ï¼Œä½†æ ‡é¢˜æ˜¯ä¸­æ–‡: '{title}'")
        
        if title_language_mismatch:
            # ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼Œç¡®ä¿è¯­è¨€ä¸€è‡´
            title = "ä»Šæ—¥è®°å½•" if is_chinese else "Today's Reflection"
            used_fallback = True
            print(f"âœ… å·²ä¿®æ­£æ ‡é¢˜ä¸º: '{title}'")
        
        if is_chinese != feedback_has_chinese:
            print(f"âš ï¸ åé¦ˆè¯­è¨€ä¸ä¸€è‡´ï¼")
            feedback = "æ„Ÿè°¢åˆ†äº«ä½ çš„è¿™ä¸€åˆ»ã€‚" if is_chinese else "Thanks for sharing this moment."
            used_fallback = True
        
        # æ¸…ç†å‡½æ•°
        def clean_text(text: str) -> str:
            text = re.sub(r'[\U0001F300-\U0001FAFF\U00002700-\U000027BF]+', '', text)
            text = text.replace('ï¼', 'ã€‚').replace('!', '.')
            text = re.sub(r'\s+', ' ', text).strip()
            return text

        def clean_text_preserve_formatting(
            text: str,
            is_chinese: bool,
            should_adjust_paragraphs: bool,
        ) -> str:
            """
            ä¿ç•™ç”¨æˆ·æ’ç‰ˆï¼ˆæ¢è¡Œ/åˆ—è¡¨ï¼‰ï¼Œåªåšè½»åº¦æ¸…ç†ã€‚
            """
            text = re.sub(r'[\U0001F300-\U0001FAFF\U00002700-\U000027BF]+', '', text)
            text = text.replace('ï¼', 'ã€‚').replace('!', '.')
            text = text.replace('\r\n', '\n').replace('\r', '\n')
            lines = text.split('\n')
            cleaned_lines = []
            bullet_pattern = re.compile(r'^(\s*)([-*â€¢]|\d+[.)])\s*(.*)$')
            for line in lines:
                if not line.strip():
                    cleaned_lines.append("")
                    continue
                bullet_match = bullet_pattern.match(line)
                if bullet_match:
                    indent, marker, content = bullet_match.groups()
                    content = re.sub(r'\s+', ' ', content).strip()
                    cleaned_lines.append(f"{indent}{marker} {content}".rstrip())
                else:
                    leading_ws = re.match(r'^\s*', line).group(0)
                    content = line[len(leading_ws):]
                    content = re.sub(r'\s+', ' ', content).strip()
                    cleaned_lines.append(f"{leading_ws}{content}".rstrip())
            cleaned = "\n".join(cleaned_lines).strip()

            if not should_adjust_paragraphs:
                return cleaned

            # å¦‚æœåŒ…å«åˆ—è¡¨ï¼Œé¿å…è‡ªåŠ¨åˆå¹¶æ®µè½
            for line in cleaned.split("\n"):
                if bullet_pattern.match(line):
                    return cleaned

            paragraphs = re.split(r"\n\s*\n+", cleaned)
            if len(paragraphs) <= 1:
                return cleaned

            def sentence_count(paragraph: str) -> int:
                if is_chinese:
                    matches = re.findall(r"[ã€‚ï¼ï¼Ÿ!?ï¼›;]", paragraph)
                else:
                    matches = re.findall(r"[.!?;]", paragraph)
                return max(1, len(matches))

            def merge_text(a: str, b: str) -> str:
                if not a:
                    return b
                if is_chinese:
                    sep = ""
                else:
                    sep = "" if a.endswith((" ", "\n")) else " "
                return f"{a.rstrip()}{sep}{b.lstrip()}"

            # âœ… é¦–æ®µè‡³å°‘åŒ…å«3å¥ï¼Œé¿å…ç¬¬ä¸€å¥åæ–­æ®µ
            while len(paragraphs) > 1 and sentence_count(paragraphs[0]) < 3:
                paragraphs[0] = merge_text(paragraphs[0], paragraphs[1])
                paragraphs.pop(1)

            # âœ… åˆå¹¶è¿‡çŸ­æ®µè½ï¼ˆé¿å…ç©ºç™½æ„Ÿï¼‰
            min_chars = 60 if is_chinese else 90
            i = 1
            while i < len(paragraphs):
                if sentence_count(paragraphs[i]) < 2 or len(paragraphs[i]) < min_chars:
                    paragraphs[i - 1] = merge_text(paragraphs[i - 1], paragraphs[i])
                    paragraphs.pop(i)
                else:
                    i += 1

            return "\n\n".join(p.strip() for p in paragraphs)
        
        def trim_to_complete_sentences(text: str, max_len: int) -> str:
            if len(text) <= max_len:
                return text

            sentence_pattern = r"([ã€‚ï¼ï¼Ÿ.!?])(['\"\"ã€ã€)]?)\s*"
            last_end = None

            for match in re.finditer(sentence_pattern, text):
                end_pos = match.end()
                if end_pos <= max_len:
                    last_end = end_pos
                else:
                    break

            if last_end is not None:
                return text[:last_end].rstrip()

            for punct in ['ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?', 'ï¼›', ';']:
                idx = text.rfind(punct, 0, max_len + 1)
                if idx > max_len * 0.5:
                    return text[:idx + 1].rstrip()
            return text[:max_len].rstrip()
        
        # ä¿®æ­£æ ‡é¢˜
        title = clean_text(title)
        title = re.sub(r'[^\w\u4e00-\u9fff\s-]', '', title)
        title = re.sub(r'\s+', ' ', title).strip()
        
        if len(title) < self.LENGTH_LIMITS["title_min"]:
            title = "Today's Reflection" if any(ord(c) < 128 for c in original_text) else "ä»Šæ—¥è®°å½•"
        elif len(title) > self.LENGTH_LIMITS["title_max"]:
            max_len = self.LENGTH_LIMITS["title_max"]
            if ' ' in title and len(title) > max_len:
                words = title[:max_len].rsplit(' ', 1)
                title = words[0] if len(words[0]) > max_len * 0.6 else title[:max_len]
            else:
                title = title[:max_len]
        
        # ä¿®æ­£æ¶¦è‰²å†…å®¹
        original_has_linebreaks = "\n" in original_text
        original_has_list = bool(re.search(r"(?m)^\s*([-*â€¢]|\d+[.)])\s+", original_text))
        should_adjust_paragraphs = not original_has_linebreaks and not original_has_list
        polished = clean_text_preserve_formatting(
            polished,
            is_chinese,
            should_adjust_paragraphs,
        )
        max_polished_len = int(orig_len * self.LENGTH_LIMITS["polished_ratio"])
        
        # âœ… æ·»åŠ é•¿åº¦æ£€æŸ¥æ—¥å¿—
        print(f"ğŸ“Š æ¶¦è‰²å†…å®¹éªŒè¯: åŸå§‹é•¿åº¦={orig_len}, æ¶¦è‰²åé•¿åº¦={len(polished)}, æœ€å¤§å…è®¸é•¿åº¦={max_polished_len}")
        
        # âš ï¸ å¦‚æœæ¶¦è‰²åå†…å®¹æ˜æ˜¾å°‘äºåŸå§‹å†…å®¹ï¼ˆå°äº80%ï¼‰ï¼Œå¯èƒ½æ˜¯è¢«æˆªæ–­äº†ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
        if len(polished) < orig_len * 0.8:
            print(f"âš ï¸ è­¦å‘Šï¼šæ¶¦è‰²åå†…å®¹æ˜æ˜¾å°‘äºåŸå§‹å†…å®¹ï¼ˆ{len(polished)} < {orig_len * 0.8}ï¼‰ï¼Œä½¿ç”¨åŸå§‹å†…å®¹")
            polished = original_text.strip()
        
        # åªæœ‰åœ¨è¶…è¿‡æœ€å¤§é•¿åº¦æ—¶æ‰æˆªæ–­ï¼ˆä½†è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºæç¤ºè¯è¦æ±‚â‰¤115%ï¼‰
        if len(polished) > max_polished_len:
            print(f"âš ï¸ æ¶¦è‰²åå†…å®¹è¶…è¿‡æœ€å¤§é•¿åº¦ï¼ˆ{len(polished)} > {max_polished_len}ï¼‰ï¼ŒæŒ‰å®Œæ•´å¥å­æˆªæ–­")
            polished = trim_to_complete_sentences(polished, max_polished_len)
        
        # ä¿®æ­£åé¦ˆ
        feedback = clean_text(feedback)
        
        if not used_fallback and len(feedback) < self.LENGTH_LIMITS.get("feedback_min", 20):
            print(f"âš ï¸ åé¦ˆè¿‡çŸ­ï¼Œä½¿ç”¨é™çº§")
            feedback = "æ„Ÿè°¢åˆ†äº«ä½ çš„è¿™ä¸€åˆ»ã€‚" if is_chinese else "Thanks for sharing this moment."
        
        if len(feedback) > self.LENGTH_LIMITS["feedback_max"]:
            print(f"ğŸ“ åé¦ˆè¿‡é•¿ï¼ŒæŒ‰å®Œæ•´å¥å­æˆªæ–­")
            feedback = trim_to_complete_sentences(feedback, self.LENGTH_LIMITS["feedback_max"])
        
        is_english = any(ord(c) < 128 for c in original_text[:50])
        default_feedback = "Thank you for sharing." if is_english else "æ„Ÿè°¢åˆ†äº«ã€‚"
        
        return {
            "title": title,
            "polished_content": polished or original_text,
            "feedback": feedback or default_feedback,
            "emotion_data": emotion_data # âœ… è¿”å›æƒ…ç»ªæ•°æ®
        }
    
    def _create_fallback_result(self, text: str) -> Dict[str, Any]:
        """
        åˆ›å»ºé™çº§ç»“æœ
        
        ğŸ”¥ æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•å®Œå…¨ä¿æŒä¸å˜
        """
        import re
        
        print("âš ï¸ ä½¿ç”¨é™çº§æ–¹æ¡ˆ")
        
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        is_chinese = chinese_chars > len(text) * 0.2
        
        return {
            "title": "ä»Šæ—¥è®°å½•" if is_chinese else "Today's Reflection",
            "polished_content": text,
            "feedback": "æ„Ÿè°¢åˆ†äº«ã€‚" if is_chinese else "Thanks for sharing.",
            "emotion_data": {"emotion": "Reflective", "confidence": 0.5} # âœ… é»˜è®¤æƒ…ç»ª
        }
    
    # ========================================================================
    # å‘åå…¼å®¹æ–¹æ³•ï¼ˆä¿æŒä¸å˜ï¼‰
    # ========================================================================
    
    def polish_text(self, text: str) -> str:
        """æ¶¦è‰²æ–‡æœ¬ï¼ˆæ—§APIï¼‰"""
        result = self.polish_content_multilingual(text)
        return result["polished_content"]
    
    def generate_feedback(self, diary_content: str) -> str:
        """ç”Ÿæˆåé¦ˆï¼ˆæ—§APIï¼‰"""
        result = self.polish_content_multilingual(diary_content)
        return result["feedback"]


    # ========================================================================
    # ğŸ”¥ å›¾ç‰‡ä¸‹è½½å’Œç¼–ç ï¼ˆç”¨äºVision APIï¼‰
    # ========================================================================
    
    async def _download_and_encode_image(self, image_url: str) -> str:
        """
        ä¸‹è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64ç¼–ç ï¼ˆç”¨äºOpenAI Vision APIï¼‰
        
        Args:
            image_url: å›¾ç‰‡çš„URLï¼ˆS3 URLæˆ–HTTP URLï¼‰
        
        Returns:
            base64ç¼–ç çš„å›¾ç‰‡æ•°æ®
        """
        try:
            print(f"ğŸ“¥ ä¸‹è½½å›¾ç‰‡: {image_url[:50]}...")
            
            # ä¸‹è½½å›¾ç‰‡
            response = await asyncio.to_thread(requests.get, image_url, timeout=10)
            response.raise_for_status()
            
            # è½¬æ¢ä¸ºbase64
            image_base64 = base64.b64encode(response.content).decode('utf-8')
            
            print(f"âœ… å›¾ç‰‡ä¸‹è½½å¹¶ç¼–ç å®Œæˆï¼Œå¤§å°: {len(image_base64)} å­—ç¬¦")
            return image_base64
            
        except Exception as e:
            print(f"âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
            raise

# ğŸ¯ ä½¿ç”¨ç¤ºä¾‹
"""
# 1. åˆå§‹åŒ–æœåŠ¡
service = OpenAIService()

# 2. è¯­éŸ³è½¬æ–‡å­—ï¼ˆWhisperï¼‰
text = await service.transcribe_audio(audio_bytes, "recording.m4a")

# 3. å¹¶è¡Œå¤„ç†ï¼šæ¶¦è‰²ï¼ˆhaiku å­—æ®µï¼‰+ åé¦ˆï¼ˆsonnet å­—æ®µï¼‰
result = await service.polish_content_multilingual(text)

# 4. ä½¿ç”¨ç»“æœ
print(f"æ ‡é¢˜: {result['title']}")        # GPT-4o-miniï¼ˆhaiku å­—æ®µï¼‰ç”Ÿæˆ
print(f"å†…å®¹: {result['polished_content']}")  # GPT-4o-miniï¼ˆhaiku å­—æ®µï¼‰æ¶¦è‰²
print(f"åé¦ˆ: {result['feedback']}")      # GPT-4o-miniï¼ˆsonnet å­—æ®µï¼‰ç”Ÿæˆ

# 5. å›¾ç‰‡+æ–‡å­—å¤„ç†ï¼ˆæ–°åŠŸèƒ½ï¼‰
result = await service.polish_content_multilingual(
    text="ä»Šå¤©å»äº†å…¬å›­",
    image_urls=["https://s3.../image1.jpg", "https://s3.../image2.jpg"]
)
"""
